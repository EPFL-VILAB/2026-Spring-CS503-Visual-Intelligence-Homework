{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e07331-51ae-4601-8c3e-ce549cef2b64",
   "metadata": {},
   "source": [
    "# CS-503 Graded Notebook on Transformers\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this notebook is to familiarize yourself with the following topics:\n",
    "- Self-attention\n",
    "- Basic tokenization\n",
    "- Basic positional encodings\n",
    "- Transformer encoder-only (e.g. ViT) and decoder-only (e.g. GPT) models\n",
    "- Vision Transformer (ViT)\n",
    "- Supervised training\n",
    "- Autoregressive modelling\n",
    "\n",
    "This notebook will not cover:\n",
    "- Cross-attention\n",
    "- Full transformer encoder+decoder\n",
    "- Weight initialization techniques\n",
    "- Stochastic depth, dropout, etc\n",
    "- Masked or contrastive pre-training\n",
    "- More advanced tokenization techniques\n",
    "- Distributed training\n",
    "- NLP\n",
    "- Multi-modality\n",
    "- etc...\n",
    "\n",
    "This notebook should give you a solid foundation of working with Transformer models and get you \"thinking with tokens\".\n",
    "\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code (denoted by ???), run the training loops and get some results.\n",
    "- We also ask several open questions. Please limit your answers to 2-3 sentences.\n",
    "- Submit the notebook with all cells executed.\n",
    "- We should be able to \"Restart Kernel and Run All Cells\" and get similar outputs to your solution.\n",
    "- The notebooks are individual homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23d351-746b-4365-af0d-f38b2c54959b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup\n",
    "\n",
    "We highly recommend running the notebook on a GPU, e.g. by opening it on [Colab](https://colab.research.google.com/).\n",
    "That said, if you have the patience, you can also run it on regular CPUs or on Apple Silicon (neither is recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905ddcd-4fb6-455d-99e2-b2561e6ed9d2",
   "metadata": {},
   "source": [
    "### 1.1 Dependencies\n",
    "\n",
    "We assume you know how to set up Python virtual environments, install packages and use Colab. If not, please come to the exercise session for troubleshooting.\n",
    "\n",
    "This notebook requires:\n",
    "- PyTorch & torchvision\n",
    "- einops\n",
    "- matplotlib\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab39ab-bddc-401a-a877-948429479af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies. If you already have them installed, no need to run this cell.\n",
    "!pip install torch torchvision einops matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515ed22-b852-42fa-a85b-456edc5a1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from einops import rearrange, repeat\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    !nvidia-smi\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Selected device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb739-cd0b-4f4a-bacf-4f2ce3c100cd",
   "metadata": {},
   "source": [
    "### 1.2 Data\n",
    "\n",
    "To reduce the computational cost of running this notebook, we'll operate on downsized 14x14 MNIST images. \n",
    "Even though we will only work on toy problems, the principles shown in this notebook are very similar to, if not the same in real-world models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e64236-6187-4dfd-abaf-f95c795dcdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = 14\n",
    "batch_size = 64\n",
    "num_workers = 10\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((image_size, image_size)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_train_val = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset_train_val, [50_000, 10_000])\n",
    "dataset_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, drop_last=False)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745db34-c2b2-4147-840c-3384c3f08022",
   "metadata": {},
   "source": [
    "Let's look at some images from the training set (it's just good old MNIST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fae5c8-4ec7-4f13-a734-569115b71fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, cls_idxs = next(iter(loader_train))\n",
    "\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(8, 8), axes_pad=0.1)\n",
    "for i, img in enumerate(imgs):\n",
    "    grid[i].imshow(img[0], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d679e-8069-460a-8ed3-152d8f53480b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Image classification with a Vision Transformer (ViT)\n",
    "\n",
    "We will now implement a simple [ViT](https://arxiv.org/abs/2010.11929) and perform digit classification on MNIST.\n",
    "\n",
    "![Vision Transformer](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)\n",
    "\n",
    "Remember that a ViT is a Transformer encoder, which consists of several stacked blocks of self-attention and MLPs. \n",
    "We will first implement the MLP and self-attention, which can then be assembled into a Transformer encoder block.\n",
    "Afterwards, we will look at positional embeddings and the patching operation to turn images into sequences of tokens.\n",
    "\n",
    "![Transformer encoder](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)\n",
    "\n",
    "Please consult the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) and [ViT](https://arxiv.org/abs/2010.11929) papers, as well as the reading material at the bottom of this notebook and the course slides, should you require further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e887046-fe97-4bce-bc88-a6c91596702d",
   "metadata": {},
   "source": [
    "### 2.1 MLP layer (10 pts)\n",
    "\n",
    "In the following cell, implement the following two-layer Perceptron:\n",
    "\n",
    "$$ \\text{MLP}(X) = \\text{GeLU}(X W_1^T + b_1) W_2^T + b_2 $$\n",
    "\n",
    "Here, $\\text{GeLU}$ denotes the [GeLU activation function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html). \n",
    "The first linear layer projects x from dimension `dim` to `int(mlp_ratio * dim)`, while the second projects it back to `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503072a7-3b35-4bc7-bdab-25a9d60d4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO\n",
    "        ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8904a-09de-4792-b17d-b2faaf71f071",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 (Masked) self-attention layer (20 pts)\n",
    "\n",
    "Next, we ask you to implement a layer that performs (optionally masked) multi-headed self-attention.\n",
    "\n",
    "Remember the scaled dot-product attention formula seen in the course:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}_\\text{row} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "The queries $Q$, the keys $K$, and the values $V$ are all linear projections of $X$:\n",
    "\n",
    "$$ Q(X) = X W_Q^T $$\n",
    "$$ K(X) = X W_K^T $$\n",
    "$$ V(X) = X W_V^T $$\n",
    "\n",
    "The scaling factor $d_k$ is the dimensionality of the keys $K$, i.e. `dim // num_heads`.\n",
    "\n",
    "The attention is performed on `num_heads` heads in parallel (don't use a for loop) in `dim // num_heads`-dimensional subspaces and the results are concatenated along the feature dimension.\n",
    "\n",
    "In addition, we want to enable masking of the attention matrix, e.g. for implementing a Transformer decoder.\n",
    "For this, the forward function takes an additional `mask` argument, specifying where to zero-out the attention matrix.\n",
    "In practice, this is implemented by replacing the values of the attention matrix (pre softmax) to minus infinity wherever we don't want any attention flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35b40e-b6e8-4875-a078-a090e6793536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        '''\n",
    "        Self-attention layer.\n",
    "        \n",
    "        params:\n",
    "            :dim: Dimensionality of each token\n",
    "            :num_heads: Number of attention heads\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # TODO: Define here the linear layers producing K, Q, V from the input x\n",
    "        ???\n",
    "        \n",
    "        # Projection\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "        Performs a forward pass through the multi-headed self-attention layer.\n",
    "        \n",
    "        params:\n",
    "            :x: Input of shape [B N C]. B = batch size, N = sequence length, C = token dimensionality\n",
    "            :mask: Optional attention mask of shape [B N N]. Wherever it is True, the attention matrix will\n",
    "            be zero.\n",
    "            \n",
    "        returns:\n",
    "            Output of shape [B N C].\n",
    "        '''\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # TODO: Compute the keys K, queries Q, and values V from x. Each should be of shape [B num_heads N head_dim].\n",
    "        q, k, v = ???\n",
    "\n",
    "        # TODO: Compute the attention matrix (pre softmax) and scale it by 1/sqrt(d_k). It should be of shape [B num_heads N N].\n",
    "        attn = ???\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = rearrange(mask, \"b n1 n2 -> b 1 n1 n2\")\n",
    "            # TODO: Apply the optional attention mask. Wherever the mask is True, replace the attention \n",
    "            # matrix value by negative infinity → zero attention weight after softmax.\n",
    "            attn = ???\n",
    "\n",
    "        # TODO: Compute the softmax over the last dimension\n",
    "        attn = ???\n",
    "\n",
    "        # TODO: Weight the values V by the attention matrix and concatenate the different attention heads\n",
    "        x = ???\n",
    "        \n",
    "        # One final projection\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e812c7-9512-45aa-8e6f-f44d273b3169",
   "metadata": {},
   "source": [
    "### 2.3 Open ended question on attention (10 pts)\n",
    "\n",
    "#### Question:\n",
    "What are the main differences between self-attention and convolutions?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d9e95-8d26-4225-9a81-ee6fec7d694e",
   "metadata": {},
   "source": [
    "### 2.4 Transformer encoder block (10 pts)\n",
    "\n",
    "In the next cell, implement a Transformer block. It is defined as:\n",
    "\n",
    "$$ X_a = X + \\text{Attention}(\\text{LN}(X)) $$\n",
    "$$ X_b = X_a + \\text{MLP}(\\text{LN}(X_a)) $$\n",
    "\n",
    "Here, $\\text{LN}$ denotes two LayerNorm layers.\n",
    "\n",
    "Don't forget to pass the optional mask to the self-attention layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5f348-c986-49ca-9bd0-980f61afdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.):\n",
    "        '''\n",
    "        Transformer encoder block.\n",
    "        \n",
    "        params:\n",
    "            :dim: Dimensionality of each token\n",
    "            :num_heads: Number of attention heads\n",
    "            :mlp_ratio: MLP hidden dimensionality multiplier\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO\n",
    "        ???\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "        Performs a forward pass through the multi-headed self-attention layer.\n",
    "        \n",
    "        params:\n",
    "            :x: Input of shape [B N C]. B = batch size, N = sequence length, C = token dimensionality\n",
    "            :mask: Optional attention mask of shape [B N N]. Wherever it is True, the attention matrix will\n",
    "            be zero.\n",
    "            \n",
    "        returns:\n",
    "            Output of shape [B N C].\n",
    "        '''\n",
    "        \n",
    "        # TODO\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc9e29-9fa1-4e93-90cf-317a9fb84779",
   "metadata": {},
   "source": [
    "### 2.5 Patch embedding layer (10 pts)\n",
    "\n",
    "So far, we implemented everything needed to build a vanilla Transformer encoder and we could more or less directly use it to train e.g. a [BERT](https://arxiv.org/abs/1810.04805) model.\n",
    "If we want to use it for images, we first have to turn these images into sequences of tokens (since that's what Transformers operate on).\n",
    "If we were to do this naively and simply flatten an HxWxC image into (H * W) C-dimensional tokens, we quickly run into difficulties...\n",
    "Even moderately-sized images would result in massive sequence lengths. Let's say our images have size 224x224. Then, our sequence length would be 50'176. Given how the cost of self-attention scales quadratically with the sequence length, we need to find another solution.\n",
    "\n",
    "We can address this problem by spending a little bit of compute and using the implicit bias of image-like data that neighboring pixels are often highly correlated. \n",
    "[Vision Transformers](https://arxiv.org/abs/2010.11929) split images into patches (e.g. of size 16x16) and then linearly project each of them to the desired token dimension.\n",
    "The same 224x224 image from before would be turned into 14x14 patches, each of size 16x16x3, flattened to 768-dimensional vectors.\n",
    "Each of these vectors is then linearly projected to the token dimension of the Transformer.\n",
    "\n",
    "In the next cell, your task is to implement the patching + patch-wise linear projection operation.\n",
    "\n",
    "Hint: Can you perform this operation with a single Conv2d layer? Using einops can yield an alternative and elegant solution as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded45059-e83c-49da-b2b1-0e902088b9b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db2db3-ff29-4e31-be8f-b5100695d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=14, patch_size=2, in_channels=1, embed_dim=192):\n",
    "        '''\n",
    "        Image to Patch Embedding.\n",
    "        \n",
    "        params:\n",
    "            :img_size: Image height and width in pixels\n",
    "            :patch_size: Patch size height and width in pixels\n",
    "            :in_channels: Number of input channels\n",
    "            :embed_dim: Token dimension\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO\n",
    "        ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Performs a forward pass through the patch embedding.\n",
    "        \n",
    "        params:\n",
    "            :x: Input of shape [B C H W]. B = batch size, C = number of channels, H = image height, W = image width\n",
    "            \n",
    "        returns:\n",
    "            Output of shape [B N C].\n",
    "        '''\n",
    "        \n",
    "        # TODO\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54bd0e-9777-49b6-8c00-c5aabb66b2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.6 Positional embeddings\n",
    "\n",
    "Attention is a permuation invariant operation for keys and values, and equivariant for queries, meaning that if we don't \"tag\" each token with some positional information, the Transformer would have no way of keeping track of where some piece of information comes from and goes to.\n",
    "We can add this positional information by summing to each token a unique positional embedding.\n",
    "Should this information be useful for the task, then the Transformer can \"choose\" to propagate it throughout the layers and keep track of it.\n",
    "\n",
    "Just like the tokenization, positional embeddings depend on the type of data too. \n",
    "If we deal with sequences like text, 1-dimensional embeddings can be either hardcoded, or learned end-to-end.\n",
    "\n",
    "Here are examples of common sine-cosine positional embeddings in both 1D and 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615bcaf-65ba-4004-be38-99e86fa0b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_sincos_posemb(max_len, embed_dim=1024, temperature=10000.):\n",
    "    \"\"\"Sine-cosine positional embeddings from MoCo-v3, adapted back to 1d\n",
    "    Returns positional embedding of shape [1, N, D]\n",
    "    \"\"\"\n",
    "    arange = torch.arange(max_len, dtype=torch.float32)\n",
    "    assert embed_dim % 2 == 0, 'Embed dimension must be divisible by 2 for 1D sin-cos position embedding'\n",
    "    pos_dim = embed_dim // 2\n",
    "    omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
    "    omega = 1. / (temperature ** omega)\n",
    "    out = torch.einsum('m,d->md', [arange, omega])\n",
    "    pos_emb = torch.cat([torch.sin(out), torch.cos(out)], dim=1)[None, :, :]\n",
    "    return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295372b-089b-4208-9bdf-7ebbf6bd7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length 49, with token dimensionality 192\n",
    "plt.imshow(build_1d_sincos_posemb(49,192)[0], cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a8f0a-7101-4bed-86ef-52c124e9a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2d_sincos_posemb(h, w, embed_dim=1024, temperature=10000.):\n",
    "    \"\"\"Sine-cosine positional embeddings from MoCo-v3\n",
    "    Source: https://github.com/facebookresearch/moco-v3/blob/main/vits.py\n",
    "    Returns positional embedding of shape [B, N, D]\n",
    "    \"\"\"\n",
    "    grid_w = torch.arange(w, dtype=torch.float32)\n",
    "    grid_h = torch.arange(h, dtype=torch.float32)\n",
    "    grid_w, grid_h = torch.meshgrid(grid_w, grid_h)\n",
    "    assert embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n",
    "    pos_dim = embed_dim // 4\n",
    "    omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
    "    omega = 1. / (temperature ** omega)\n",
    "    out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])\n",
    "    out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])\n",
    "    pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]\n",
    "    return pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa585cc-4acd-484c-a409-61d52fe6b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length 49 (= 7x7 patches), with token dimensionality 192\n",
    "plt.imshow(build_2d_sincos_posemb(7,7,192)[0], cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00283495-430f-43b8-adfb-4af7cc48b000",
   "metadata": {},
   "source": [
    "### 2.7 Assembling the blocks into a ViT (10 pts)\n",
    "\n",
    "Now we have all the building blocks to create a ViT! \n",
    "\n",
    "Implement the following operations:\n",
    "- Turning images into patches and projecting them to the token dimension\n",
    "- Adding positional embeddings to the tokens\n",
    "- Running the tokens through all Transformer blocks\n",
    "- Taking the mean over all tokens of each batch and projecting to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3fb60-74c3-4ecf-8b9b-baf6530b37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 img_size=14, \n",
    "                 patch_size=2, \n",
    "                 in_channels=1, \n",
    "                 embed_dim=192, \n",
    "                 num_classes=10, \n",
    "                 depth=4, \n",
    "                 num_heads=4, \n",
    "                 mlp_ratio=4., \n",
    "                 **kwargs):\n",
    "        '''\n",
    "        A Vision Transformer for classification.\n",
    "        \n",
    "        params:\n",
    "            :img_size: Image height and width in pixels\n",
    "            :patch_size: Patch size height and width in pixels\n",
    "            :in_channels: Number of input channels\n",
    "            :embed_dim: Token dimension\n",
    "            :num_classes: Number of classes\n",
    "            :depth: Transformer depth\n",
    "            :num_heads: Number of attention heads\n",
    "            :mlp_ratio: MLP hidden dimensionality multiplier\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            build_2d_sincos_posemb(image_size//patch_size, image_size//patch_size, embed_dim=embed_dim), \n",
    "            requires_grad=False\n",
    "        )\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Linear(embed_dim, num_classes, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass through the ViT.\n",
    "        \n",
    "        params:\n",
    "            :x: Input of shape [B C H W]. B = batch size, C = number of channels, H = image height, W = image width\n",
    "            \n",
    "        returns:\n",
    "            Output of shape [B num_classes]\n",
    "        '''        \n",
    "        # TODO: Project images to patches\n",
    "        ???\n",
    "        \n",
    "        # TODO: Add the positional embeddings to the tokens\n",
    "        ???    \n",
    "            \n",
    "        # TODO: Forward pass through Transformer blocks\n",
    "        ???\n",
    "            \n",
    "        # TODO: Perform average pooling (compute the mean over the sequences)\n",
    "        ???\n",
    "        \n",
    "        # TODO: Compute the logits\n",
    "        ???\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e31c1-f1cc-4f71-81c7-0d049a60f6f2",
   "metadata": {},
   "source": [
    "### 2.8 Training the ViT for digit classification\n",
    "\n",
    "Finally, let's train a very small ViT on MNIST. Here's a simple training loop that should only take 5-10 minutes on a V100 or Tesla T4 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982ad3d-1300-4de5-b714-76e137ec1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    img_size=14, patch_size=2, in_channels=1, \n",
    "    embed_dim=192, num_classes=10, depth=4, \n",
    "    num_heads=4, mlp_ratio=4., \n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(vit.parameters())\n",
    "num_parameters = sum([p.numel() for p in vit.parameters()])\n",
    "print(f'Number of parameters: {num_parameters:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5df51-b12e-4d3d-bf2b-ef1b06a498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    \n",
    "    # Train loop\n",
    "    vit.train()\n",
    "    epoch_loss_train = 0\n",
    "    pbar = tqdm(total=len(loader_train))\n",
    "    for imgs, cls_idxs in loader_train:\n",
    "        inputs, targets = imgs.to(device), cls_idxs.to(device)\n",
    "\n",
    "        logits = vit(inputs)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        vit.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_train += loss.item()\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Train loss: {loss.item():.3f}')\n",
    "    pbar.close()\n",
    "    \n",
    "    epoch_loss_train /= len(loader_train)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    # Validation loop\n",
    "    vit.eval()\n",
    "    epoch_loss_val = 0\n",
    "    for imgs, cls_idxs in loader_val:\n",
    "        inputs, targets = imgs.to(device), cls_idxs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = vit(inputs)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        epoch_loss_val += loss.item()\n",
    "    \n",
    "    epoch_loss_val /= len(loader_val)\n",
    "    val_losses.append(epoch_loss_val)\n",
    "    \n",
    "    print(f'Train loss: {epoch_loss_train:.3f}, val loss: {epoch_loss_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2321f-a8ae-4995-882a-997d47976989",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(val_losses, label='Val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50faa034-c780-40ab-8721-c11aaeb1be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "vit.eval()\n",
    "for imgs, cls_idxs in loader_test:\n",
    "    inputs, targets = imgs.to(device), cls_idxs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = vit(inputs)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    test_loss += loss.item()\n",
    "    \n",
    "    pred = logits.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(loader_test)\n",
    "accuracy = correct / len(loader_test.dataset)\n",
    "\n",
    "print(f'Test loss: {test_loss:.3f}')\n",
    "print(f'Test top-1 accuracy: {accuracy*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d59c82-cb03-489c-8f4b-c33460fbaa1d",
   "metadata": {},
   "source": [
    "For reference, we get over 97% top-1 test set accuracy with this simple training loop over 20 epochs. Some stochasticity around that value is expected, but if you're far below, check your implementation again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ab07c-3ef0-4586-b7a2-72ee757c75d4",
   "metadata": {},
   "source": [
    "### 2.9 Open ended question on Vision Transformers (10 pts)¶\n",
    "\n",
    "#### Question:\n",
    "This ViT was designed for image classificaton. How could we modify it for solving dense prediction tasks, such as pixel-wise depth estimation or semantic segmentation?\n",
    "\n",
    "#### Answer:\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac016e02-5231-4716-b31b-0d99f35b25d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Image generation with GPT\n",
    "\n",
    "In this part, we will implement a GPT model and train it on images, similar in spirit to [iGPT](https://openai.com/research/image-gpt).\n",
    "What this means is that we turn images into a sequence of discrete tokens and train a decoder-only Transformer to perform next-token-prediction.\n",
    "At inference time, we can then autoregressively generate new images from the training distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b72c7-5151-448e-a580-ec695fa92924",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Tokenization\n",
    "\n",
    "First, we need to turn images into sequences of discrete tokens.\n",
    "\n",
    "A common way of performing tokenization of images is to train a discrete VAE, such as (VQ-GAN)[https://arxiv.org/abs/2012.09841] to autoencode images through a discrete bottleneck.\n",
    "Since we don't want to open that can of worms in a homework about Transformers, we can instead suggest you to check this out in your own time.\n",
    "\n",
    "Here, we will tokenize the images in a much simpler way:\n",
    "- First, we turn the grayscale images into black and white → each image is now a sequence of 14x14 zeros and ones. In this manner, we would already have a sequence of discrete tokens we could model, but the sequence length of 14x14=192 could be shorter.\n",
    "- To reduce the sequence length further, we divide the image into 2x2 patches and turn each of the 2x2 patterns into a unique index. If we flatten each 2x2 patch into a sequence of 4 zeros and ones, we can directly interpret them as integers between 0 and 15. This reduces the sequence length by a factor of four. Much more manageable in our toy setting!\n",
    "\n",
    "While this way of tokenizing is really quite \"toy\", it is indicative of a common problem when dealing with images and transformers: naively turning images to tokens can result in very large sequence lengths, which do not go well with the quadratic complexity of self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4688cb-d86a-44b4-aaef-d2f3bd681cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec2bin(x, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).float()\n",
    "\n",
    "def bin2dec(b, bits):\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(b.device, b.dtype)\n",
    "    return torch.sum(mask * b, -1).long()\n",
    "\n",
    "def tokenize_MNIST(imgs, patch_size=2):\n",
    "    imgs = (imgs[:,0] > 0.5).int()\n",
    "    bits = rearrange(\n",
    "        imgs, \n",
    "        'b (nh ph) (nw pw) -> b (nh nw) (ph pw)', \n",
    "        ph=patch_size, pw=patch_size\n",
    "    )\n",
    "    return bin2dec(bits, patch_size**2)\n",
    "\n",
    "def detokenize_MNIST(imgs_tokenized, patch_size=2):\n",
    "    bits = dec2bin(imgs_tokenized, patch_size**2)\n",
    "    N = int(math.sqrt(imgs_tokenized.shape[-1]))\n",
    "    return rearrange(\n",
    "        bits, \n",
    "        'b (nh nw) (ph pw) -> b (nh ph) (nw pw)', \n",
    "        ph=patch_size, pw=patch_size, nh=N, nw=N\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f65314-fdd2-419f-b006-eb05d6884e98",
   "metadata": {},
   "source": [
    "Here are some examples of tokenized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf85f2c-a8b3-4810-9f70-cacd3c3a6361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs, _ = next(iter(loader_train))\n",
    "\n",
    "tokens = tokenize_MNIST(imgs, patch_size=2)\n",
    "reconst = detokenize_MNIST(tokens, patch_size=2)\n",
    "bits = rearrange(reconst, 'b (nh ph) (nw pw) -> b (nh nw) ph pw', ph=2, pw=2)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(5., 5.)); plt.imshow(reconst[i], cmap='Greys'); plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(7, 7), axes_pad=0.1)\n",
    "    for j, img in enumerate(bits[i]):\n",
    "        grid[j].imshow(img, cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Tokens:', tokens[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf128e-357c-4a80-a2d7-0be4c54b09e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 GPT implementation (20 pts)\n",
    "\n",
    "In the next cell, implement a GPT model. Specifically, implement the following:\n",
    "- Embed the token indices into the desired Transformer token dimension\n",
    "- Add to each token a unique positional embedding\n",
    "- Create a causal mask that restricts the self-attention to the same and previous tokens\n",
    "- Run the tokens through the Transformer blocks\n",
    "- Compute per-token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e2633-b6c2-4e74-897a-69a12cb2bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, max_tokens=49+1, vocab_size=16+2, embed_dim=192, depth=4, num_heads=4, mlp_ratio=4., **kwargs):\n",
    "        '''\n",
    "        GPT to autoregressively model sequences.\n",
    "        \n",
    "        params:\n",
    "            :max_tokens: Maximum sequence length (should include start-of-sequence token)\n",
    "            :vocab_size: Vocabulary size (should include start-of-sequence and end-of-sequence token)\n",
    "            :embed_dim: Token dimension\n",
    "            :depth: Transformer depth\n",
    "            :num_heads: Number of attention heads\n",
    "            :mlp_ratio: MLP hidden dimensionality multiplier\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        # For simplicity, we will use a 1D positional embedding here, even though the data is 2D.\n",
    "        self.pos_embed = nn.Parameter(build_1d_sincos_posemb(max_tokens, embed_dim=embed_dim), requires_grad=False)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass through the GPT.\n",
    "        \n",
    "        params:\n",
    "            :x: Input of shape [B N]. B = batch size, N = sequence length\n",
    "            \n",
    "        returns:\n",
    "            Output of shape [B N vocab_size]\n",
    "        '''\n",
    "        B, N = x.shape\n",
    "        \n",
    "        # TODO: Embed the input x\n",
    "        ???\n",
    "        \n",
    "        # TODO: Add to x the positional embeddings. Make sure to deal with the case, where N is smaller than the maximum sequence length.\n",
    "        ???\n",
    "        \n",
    "        # TODO: Create a causal mask of shape [B N N]. The mask should set values in the attention matrix \n",
    "        # to zero, such that tokens can only attend to previous tokens or themselves, but not at \"future\" tokens.\n",
    "        ???\n",
    "        \n",
    "        # TODO: Forward pass through Transformer blocks. Make sure to pass the causal mask.\n",
    "        ???\n",
    "            \n",
    "        # TODO: Compute the logits for each token\n",
    "        ???\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, seq, max_new_tokens, temperature=1.0, do_sample=True, top_k=None):\n",
    "        '''\n",
    "        Given a start sequence, generate the remaining tokens.\n",
    "        \n",
    "        params:\n",
    "            :seq: Input sequence of shape [B n]. To generate samples from scratch, pass a [B 1] tensor full of the \"start sequence\" token.\n",
    "            :max_new_tokens: Number of tokens to generate\n",
    "            :temperature: Sample tokens with the desired temperature\n",
    "            :do_sample: Set to True to sample and False for simply picking the most likely token\n",
    "            :top_k: Optionally restrict the logits to top k options\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(seq)\n",
    "             # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            seq = torch.cat((seq, idx_next), dim=1)\n",
    "            \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5819551-f63e-4014-bbac-479a2b491d17",
   "metadata": {},
   "source": [
    "### 3.5 Train iGPT\n",
    "\n",
    "Let's train the GPT on MNIST and generate some samples!\n",
    "We prepend every sequence with a start-of-sequence token and append an end-of-sequence token.\n",
    "We train the Decoder-only transformer to predict a shifted version of the input and thanks to the causal mask, tokens can only attend to previous tokens and themselves.\n",
    "\n",
    "Don't worry if not all of the generated images look like MNIST digits, though most of them should look somewhat plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31a235-1f40-4cc2-b3ea-12da5072e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 2\n",
    "vocab_size = 2**(patch_size*patch_size)\n",
    "max_tokens = (image_size//patch_size)**2\n",
    "\n",
    "gpt = GPT(\n",
    "    max_tokens=max_tokens+1, # number of tokens for one image + one for start-of-sequence token\n",
    "    vocab_size=vocab_size+2, # two additional vocabulary items for start-of-sequence and end-of-sequence token\n",
    "    embed_dim=192, \n",
    "    depth=4, \n",
    "    num_heads=4, \n",
    "    mlp_ratio=4.\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91c11d-2c7b-420c-b820-a2b056fcedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "start_of_seq_token = vocab_size\n",
    "end_of_seq_token = vocab_size+1\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    \n",
    "    # Train loop\n",
    "    vit.train()\n",
    "    epoch_loss_train = 0\n",
    "    pbar = tqdm(total=len(loader_train))\n",
    "    for imgs, _ in loader_train:\n",
    "        x = tokenize_MNIST(imgs, patch_size=patch_size).to(device)\n",
    "        inputs = torch.nn.functional.pad(x.clone(), (1,0,0,0), value=start_of_seq_token)\n",
    "        targets = torch.nn.functional.pad(x.clone(), (0,1,0,0), value=end_of_seq_token)\n",
    "\n",
    "        logits = gpt(inputs)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        gpt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss_train += loss.item()\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Train loss: {loss.item():.3f}')\n",
    "    pbar.close()\n",
    "    \n",
    "    epoch_loss_train /= len(loader_train)\n",
    "    train_losses.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    # Validation loop\n",
    "    vit.eval()\n",
    "    epoch_loss_val = 0\n",
    "    for imgs, _ in loader_val:\n",
    "        x = tokenize_MNIST(imgs, patch_size=patch_size).to(device)\n",
    "        inputs = torch.nn.functional.pad(x.clone(), (1,0,0,0), value=start_of_seq_token)\n",
    "        targets = torch.nn.functional.pad(x.clone(), (0,1,0,0), value=end_of_seq_token)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = gpt(inputs)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        epoch_loss_val += loss.item()\n",
    "    \n",
    "    epoch_loss_val /= len(loader_val)\n",
    "    val_losses.append(epoch_loss_val)\n",
    "    \n",
    "    print(f'Train loss: {epoch_loss_train:.3f}, val loss: {epoch_loss_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1f200-d220-41db-87ff-5af03fa78d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = repeat(torch.LongTensor([[start_of_seq_token]]).to(device), '1 d -> n d', n=100)\n",
    "gen_seq = gpt.generate(seq, 49, temperature=0.7, do_sample=True).detach().cpu()\n",
    "reconst = detokenize_MNIST(gen_seq[:,1:], patch_size=patch_size)\n",
    "\n",
    "fig = plt.figure(figsize=(15., 15.))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(10, 10), axes_pad=0.1)\n",
    "for i in range(100):\n",
    "    grid[i].imshow(reconst[i], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0adbae-2808-4667-8206-268e16748753",
   "metadata": {},
   "source": [
    "## 5 Further reading\n",
    "\n",
    "Having implemented self-attention, as well as encoder-only and decoder-only Transformers, you should not have a hard time implementing cross-attention and a full encoder-decoder Transformer to train on arbitrary sequence-to-sequence tasks.\n",
    "That said, here is some further reading material should you want to dive deeper.\n",
    "\n",
    "### Papers & Blogs\n",
    "\n",
    "- [Attention Is All You Need, Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)\n",
    "- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al. 2020](https://arxiv.org/abs/2010.11929)\n",
    "- [DeiT III: Revenge of the ViT](https://arxiv.org/abs/2204.07118) → a more modern and simplified training recipe for supervised ViTs\n",
    "- [Transformer Circuits Thread](https://transformer-circuits.pub/)\n",
    "- [The Transformer Family Version 2.0, Lilian Weng 2023](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [The Illustrated Transformer, Jay Alammar 2018](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "### Code bases\n",
    "\n",
    "If you want to use transformers in your project, we recommend taking a look at the following codebases:\n",
    "- [Self-Supervised Vision Transformers with DINO](https://github.com/facebookresearch/dino)\n",
    "- [Masked Autoencoders Are Scalable Vision Learners (MAE)](https://github.com/facebookresearch/mae)\n",
    "\n",
    "You might find that, in many ways, these repos are not that much different from what we implemented here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
