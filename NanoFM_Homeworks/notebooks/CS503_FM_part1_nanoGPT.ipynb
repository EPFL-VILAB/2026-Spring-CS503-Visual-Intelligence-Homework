{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS 503 Foundation Models: Part 1 - nanoGPT\n",
    "\n",
    "In this part, we start by implementing the necessary building blocks to construct an autoregressive Transformer, like GPT.\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this first part is to familiarize yourself with the following topics:\n",
    "- Causal attention\n",
    "- Transformer decoder-only (e.g. GPT, LLaMA, ...) models\n",
    "- Basic tokenization\n",
    "- Basic positional encodings\n",
    "- Autoregressive modelling on text and images\n",
    "- Autoregressive inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with autoregressive Transformer models and get you \"thinking with tokens\".\n",
    "\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the accompanying codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed.\n",
    "- The notebooks are individual homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe433a2",
   "metadata": {},
   "source": [
    "Please follow the instructions in the [README.md](../README.md) file to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.vision.tokenized_mnist import create_tokenized_mnist_dataloader, detokenize_MNIST\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77511210-e44d-4ec6-8184-984b44db9ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Training nanoGPT on TinyStories for text generation\n",
    "\n",
    "In this exercise, we will implement a simplified autoregressive Transformer, similar to nanoGPT. \n",
    "We will train it on [TinyStories](https://arxiv.org/abs/2305.07759), a synthetically generated dataset of somewhat simple children's book stories. That focus allows us to train relatively small models that can generate coherent text and demonstrate some basic world knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eba328-3fb2-4d47-a950-204d8df454ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Overview and tasks\n",
    "\n",
    "To implement nanoGPT, we ask you to complete the subsections below by directly filling in the missing lines in the code base.\n",
    "\n",
    "Hint: After completing the implementation of nanoGPT, in case you are still debugging, you may want to run the image generation examples in section 3 first. They are significantly faster to train and may facilitate faster debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6074f1-639a-44f8-9e3f-067361272b70",
   "metadata": {},
   "source": [
    "#### 2.1.1 MLP layer\n",
    "\n",
    "In `nanofm.modeling.transformer_layers.Mlp`, implement the following two-layer Perceptron:\n",
    "\n",
    "$$ \\text{MLP}(X) = \\text{GeLU}(X W_1^T + b_1) W_2^T + b_2 $$\n",
    "\n",
    "Here, $\\text{GeLU}$ denotes the [GeLU activation function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html). \n",
    "The first linear layer projects x from dimension `in_features` to `hidden_features`, while the second projects it back to `out_features`.\n",
    "Commonly, the bias terms are disabled. Make sure to take into account the `bias` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d429c-2ae2-4b45-baad-bfc91406d6a4",
   "metadata": {},
   "source": [
    "#### 2.1.2 (Masked) self-attention layer\n",
    "\n",
    "Next, we ask you to implement a layer that performs (optionally masked) multi-headed self-attention in `nanofm.modeling.transformer_layers.Attention`.\n",
    "\n",
    "Remember the scaled dot-product attention formula:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}_\\text{row} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "The queries $Q$, the keys $K$, and the values $V$ are all linear projections of $X$:\n",
    "\n",
    "$$ Q(X) = X W_Q^T $$\n",
    "$$ K(X) = X W_K^T $$\n",
    "$$ V(X) = X W_V^T $$\n",
    "\n",
    "The scaling factor $d_k$ is the dimensionality of the keys $K$, i.e. `dim // num_heads`.\n",
    "\n",
    "The attention is performed on `num_heads` heads in parallel (don't use a for loop) in `head_dim = dim // num_heads`-dimensional subspaces and the results are concatenated along the feature dimension.\n",
    "\n",
    "In addition, we want to enable masking of the attention matrix, e.g. for implementing a Transformer decoder.\n",
    "For this, the forward function takes an additional `mask` argument, specifying where to zero-out the attention matrix.\n",
    "In practice, this is implemented by replacing the values of the attention matrix (pre softmax) to minus infinity wherever we don't want any attention flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b50df-d7f8-42af-a49d-bdd1c38aa922",
   "metadata": {},
   "source": [
    "#### 2.1.3 Transformer block\n",
    "\n",
    "Next, implement a Transformer block in `nanofm.modeling.transformer_layers.Block`. It is defined as:\n",
    "\n",
    "$$ X_a = X + \\text{Attention}(\\text{LN}(X)) $$\n",
    "$$ X_b = X_a + \\text{MLP}(\\text{LN}(X_a)) $$\n",
    "\n",
    "Here, $\\text{LN}$ denotes (two separate) LayerNorm layers.\n",
    "\n",
    "Don't forget to pass the optional mask to the self-attention layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b2a47-bd05-49b7-a2a9-858ae9e39bc6",
   "metadata": {},
   "source": [
    "#### 2.1.4 Assembling the blocks into a Transformer tunk\n",
    "\n",
    "Now we have all the building blocks to create a Transformer trunk! \n",
    "\n",
    "In `nanofm.modeling.transformer_layers.TransformerTrunk`, create an `torch.nn.ModuleList` containing multiple Transformer blocks, and in the forward pass call them one after another.\n",
    "Again, make sure to pass the mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184c029-93e6-48a6-ae2a-2eb5421e598e",
   "metadata": {},
   "source": [
    "#### 2.1.5 Initialize nanoGPT, implement the forward function, and loss (20 points)\n",
    "\n",
    "Finally, we can use this Transformer trunk to build a nanoGPT model, which we implement in `nanofm.models.gpt.GPT`.\n",
    "It consists of a few operations executed in series. Initialize the following modules in the constructor:\n",
    "1. The discrete input tokens are embedded with an `nn.Embedding` layer. Initialize `self.input_embedding` accordingly, taking into account the vocabulary size.\n",
    "2. On top of that, we add learnable positional embeddings. Initialize `self.positional_embedding` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`max_seq_len`, `dim`).\n",
    "3. This then gets passed to a Transformer trunk. Initialize `self.trunk` with the trunk you just implemented.\n",
    "4. Finally we project the trunk output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size (as a one-hot vector per token). Initialize `self.out_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False.\n",
    "\n",
    "Next, let's implement the `forward_model` function:\n",
    "1. Pass the input tokens through the embedding, add the positional embedding (make sure to account for the length of the inputs!), pass it through the Transformer trunk, output normalization, and output projection.\n",
    "2. When calling the Transformer trunk, make sure to pass a causal attention mask of shape (1, L, L), where L is the sequence length. The mask is of boolean type, and wherever it is False the attention is masked-out (i.e. set to -infinity), and otherwise it is left untouched. Remember the shape of the attention mask.\n",
    "\n",
    "Finally, we need to compute the cross-entropy loss between the logits and the ground-truth targets. Please complete the `compute_ce_loss` function accordingly, and take into account the padding token. We do not want to compute a loss on those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f29901-8554-4cf8-b1b2-d10e5b11cc5c",
   "metadata": {},
   "source": [
    "#### 2.1.6 Write the generation loop (20 points)\n",
    "\n",
    "Now we could theoretically train a model, but it would be of no use without a `generate` function. \n",
    "In this function you will run the model in a loop:\n",
    "1. Given the context so far, run the forward function to get the logits. \n",
    "2. Extract the logits of only the last token (it's the next token prediction we care about). \n",
    "3. Sample from the probability distribution specified by the last token logits. You can use the helper function `nanofm.utils.sampling.sample_tokens` for that. Make sure to pass the temperature, top-k, and top-p arguments.\n",
    "4. Concatenate the predicted next token with the context. This will be your new context for the next round. \n",
    "5. Respect the halting conditions: A) If you reach the maximum sequence length, stop generating. Take into account the length of the context so far! B) (Optional) In case you generate an `[EOS]` token corresponding to the end-of-sequence, stop generating early.\n",
    "6. Finally after doing the entire loop and halting, return the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b6865-bfc5-4b6e-8bcd-5b242d582b0e",
   "metadata": {},
   "source": [
    "#### 2.1.7 Text tokenizer overview\n",
    "\n",
    "For this part, you will not need to implement anything. This is to familiarize you with the text tokenizer we use.\n",
    "A text tokenizer's job is to take any text and turn it into a sequence of integers (i.e. encode it) that we can predict autoregressively, and to then turn that original or a predicted sequence back into text.\n",
    "Text tokenizers come in many shapes and forms, but commonly they turn subwords into a unique token, using a vocabulary of a pre-determined size. \n",
    "\n",
    "For nanoGPT, we will use the [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) tokenizer. It has a base vocabulary size of 50257, but we will add three more special tokens to it:\n",
    "- `[PAD]`: Not all samples have the same length, but in order to batch them we need to equalize their lenght. The simplest way to do this is to pad the sequences up to some maximum sequence length, e.g. 256 tokens, using special `[PAD]` tokens. During training, we do not compute a loss on these tokens.\n",
    "- `[SOS]`: The first token could just be the first token of the text sequence, but we want to be able to perform *unconditional* generation, i.e. we want to be able to generate the entire sequence and not have to give it a first token to start generating. The start-of-sequence token `[SOS]` serves as that marker. We prepend it to every sequence.\n",
    "- `[EOS]`: Given that not all samples have the same length, we need a way for the model to indicate that it is done generating the next tokens. The end-of-sequence `[EOS]` token serves that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c5fb8-88b2-40a8-ab50-7a7ef878e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "\n",
    "# Add padding, start-of-sequence, and end-of-sequence tokens\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '[SOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "})\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[('[EOS]', tokenizer.eos_token_id), ('[SOS]', tokenizer.bos_token_id)],\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d476-9aad-42b1-a28b-8b486372e6de",
   "metadata": {},
   "source": [
    "Let's see what happens when we encode sentences of variable length. Remember, we wrap the texts with an `[SOS]` (index 50258) and `[EOS]` (index 50259) token, and pad to the maximum sequence length with a `[PAD]` (index 50257) token. If a text is longer than the specified maximum sequence length, we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94471d2-edca-4ed2-8009-eee809689df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'This is an example.',\n",
    "    'Once upon a time there was a quick brown fox.',\n",
    "]\n",
    "tokens = tokenizer(texts, max_length=10, padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2644e6-cf9d-44af-8cdc-ae6adc4edf61",
   "metadata": {},
   "source": [
    "We can use the same tokenizer to turn the sequences back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468482ba-b525-416d-8f4f-6335240f837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_seq in tokens:\n",
    "    print(tokenizer.decode(token_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a75525-3ab5-4d39-99ad-99d5b6105ea0",
   "metadata": {},
   "source": [
    "Let's define a helper function to filter out the special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47251f7-7c89-4c6d-b827-2ee4f9c31bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, text_tokenizer):\n",
    "    \"\"\" Helper function to turn token sequences back to well-formatted text. \"\"\"\n",
    "    decoded = text_tokenizer.decode(token_ids)\n",
    "    # Remove [SOS], [EOS], and [PAD] tokens along with surrounding horizontal whitespace only.\n",
    "    decoded = re.sub(r'[ \\t]*\\[(SOS|EOS|PAD)\\][ \\t]*', ' ', decoded)\n",
    "    # Collapse extra horizontal spaces in each line without touching newline characters.\n",
    "    decoded = '\\n'.join([re.sub(r'[ \\t]+', ' ', line).strip() for line in decoded.splitlines()])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e5fce-30e9-441f-9343-519c95a5bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_seq in tokens:\n",
    "    print(token_ids_to_text(token_seq, text_tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99c198-7515-43fc-9c26-3f08388ab392",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/tinystories_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 2xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 run_training.py --config cfgs/nanoGPT/tinystories_d8w512.yaml\n",
    "```\n",
    "\n",
    "During training the script saves intermediate checkpoints at regular intervals. In case your training crashes, the training automatically resumes from that last checkpoint.\n",
    "In case you don't want to resume training and instead start over from scratch, please either delete the checkpoint directory of that run (e.g. in `./outputs/nanoGPT/tinystories_d8w512/`) or rename it.\n",
    "\n",
    "This training should take over one hour. You should reach a final validation loss around 1.3, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_tinystories.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94b2be-6984-4c5d-8f70-e0822b153b01",
   "metadata": {},
   "source": [
    "### 2.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "[Note] Your screenshot must clearly show your W&B account (username/entity), usually visible in the top-right corner of the page.\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbab3a-bb76-4e5d-bc78-b74bf6063dd7",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe67e53-b62f-4664-98b3-e6a8471c82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78151077-58e0-4499-85f1-92122d6ffd25",
   "metadata": {},
   "source": [
    "Let's generate some random (unconditional) stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c05a6-fd08-4fe9-bdbe-86f61a1d6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    output = model.generate(context=[tokenizer.bos_token_id], temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a84c-7727-4c17-acb5-4b415531726c",
   "metadata": {},
   "source": [
    "We don't have to generate the stories in a completely unconditional way. Let's try to probe the model and see if it learned some world knowledge. \n",
    "If we probe the model with the phrase `Daisy was hungry, so she`, we should expect a continuation talking about her getting food. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89eced-7cfd-42ef-a0d9-26758f7c543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = tokenizer.encode('Daisy was hungry, so she')[:-1] # Encode and discard automatically added [EOS] token\n",
    "\n",
    "for _ in range(5):\n",
    "    output = model.generate(context=context, temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb687d48-e5ad-48a2-9e5b-e01099505d61",
   "metadata": {},
   "source": [
    "### 2.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q2.1: What effect does the temperature have on the generations?\n",
    "    - A2.1: [Please fill your answer here]\n",
    "- Q2.2: What about the top_k and top_p parameters?\n",
    "    - A2.2: [Please fill your answer here]\n",
    "- Q2.3: Sometimes the generations of this model are not very good. What could we do to improve it further?\n",
    "    - A2.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ff7f9-f249-4412-a3f6-e60310db279a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3 Training nanoGPT on MNIST for conditional image generation\n",
    "\n",
    "In this part, we will train our nanoGPT on images, similar in spirit to [iGPT](https://openai.com/research/image-gpt).\n",
    "What this means is that we turn images into a sequence of discrete tokens and train a decoder-only Transformer to perform next-token-prediction.\n",
    "At inference time, we can then autoregressively generate new images from the training distribution, conditioned on the class label!\n",
    "\n",
    "If you have successfully completed the nanoGPT implementation, you can directly re-use it for this step! The only thing we change is the way we represent the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d1209-9135-4254-929f-723b52862259",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization\n",
    "\n",
    "First, we need to turn images into sequences of discrete tokens.\n",
    "\n",
    "A common way of performing tokenization of images is to train a discrete VAE, such as [VQ-GAN](https://arxiv.org/abs/2012.09841) to autoencode images through a discrete bottleneck. This is an entire research field of its own, so we will not go further into that here.\n",
    "\n",
    "Instead, we will tokenize the images in a much simpler way:\n",
    "- First, we turn the grayscale images into black and white â†’ each image is now a sequence of 14x14 zeros and ones. In this manner, we would already have a sequence of discrete tokens we could model, but the sequence length of 14x14=192 could be shorter.\n",
    "- To reduce the sequence length further, we divide the image into 2x2 patches and turn each of the 2x2 patterns into a unique index. If we flatten each 2x2 patch into a sequence of 4 zeros and ones, we can directly interpret them as integers between 0 and 15. This reduces the sequence length by a factor of four. Much more manageable in our toy setting!\n",
    "\n",
    "While this way of tokenizing is really quite \"toy\", it is indicative of a common problem when dealing with images and transformers: naively turning images to tokens can result in very large sequence lengths.\n",
    "\n",
    "Let's plot some examples of the tokenized images. First is the original image, then we show each token, and finally we show the corresponding token sequence. \n",
    "The first token is the class-label, and the next 49 are the image tokens in raster-scan order. Their token id was shifted by 10 to account for the additional class token (we share the vocabulary with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a238c-283d-4d48-9f9b-dceb30e57729",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_tokenized_mnist_dataloader(train=False, add_label_token=True, shuffle=False)\n",
    "data_dict = next(iter(data_loader))\n",
    "\n",
    "tokens = data_dict['input_ids']\n",
    "reconst = detokenize_MNIST(tokens, patch_size=2, account_for_labels=True)\n",
    "bits = rearrange(reconst, 'b (nh ph) (nw pw) -> b (nh nw) ph pw', ph=2, pw=2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(5., 5.)); plt.imshow(reconst[i], cmap='Greys'); plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(7, 7), axes_pad=0.1)\n",
    "    for j, img in enumerate(bits[i]):\n",
    "        grid[j].imshow(img, cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Tokens:', tokens[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a78d4a-6df4-4d20-8627-a393ddf9634d",
   "metadata": {},
   "source": [
    "### 3.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/mnist_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 1xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=1 run_training.py --config cfgs/nanoGPT/mnist_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should be pretty fast and only take a few minutes. You should reach a final validation loss below 0.45, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_mnist.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92233937-b07b-4313-8004-b17e058ca380",
   "metadata": {},
   "source": [
    "### 3.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "[Note] Your screenshot must clearly show your Weights & Biases (W&B) account (username/entity), usually visible in the top-right corner of the page.\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdd8d6-a5fe-46a3-ba4e-7ee872d495ff",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede1ce4-dfff-4a99-aa8d-1761f2141c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './outputs/nanoGPT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54f04f-d106-4cef-b23b-936a91854bd9",
   "metadata": {},
   "source": [
    "Let's plot some class-conditional generations! We seed the generation by providing the first token, whose index is equal to the number we'd like to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989eecc3-7c72-4a7f-a541-862ab263bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 5\n",
    "output = model.generate(context=[label], temp=0.7, top_p=0.0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a41606-6524-47e4-90ea-ab0b271f0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "plt.imshow(reconst[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0c051-cf4f-42ba-ba02-716ad3d2add2",
   "metadata": {},
   "source": [
    "Let's now generate 10 random samples for all 10 classes. Most should look quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0356c-eb7e-4cab-b242-27525d6a3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, temp=1.0, top_p=0.0, top_k=0.0, n_samples=10):\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(10, n_samples), axes_pad=0.1)\n",
    "    for label in range(10):\n",
    "        for sample_idx in range(n_samples):\n",
    "            grid_idx = label * n_samples + sample_idx\n",
    "            output = model.generate(context=[label], temp=temp, top_p=top_p, top_k=top_k)\n",
    "            reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "            grid[grid_idx].imshow(reconst[0], cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "generate_samples(model, temp=0.7, top_p=0.9, top_k=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29571b8-ebaf-4ef1-8426-4863e986fff6",
   "metadata": {},
   "source": [
    "### 3.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q3.1: What effect does the temperature have on the generations?\n",
    "    - A3.1: [Please fill your answer here]\n",
    "- Q3.2: What about the top_k and top_p parameters?\n",
    "    - A3.2: [Please fill your answer here]\n",
    "- Q3.3: How might we extend this to text-to-image?\n",
    "    - A3.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8908f8-c491-4519-88c3-72da701e6933",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 Further reading\n",
    "\n",
    "Having implemented causal-attention, as well as a decoder-only Transformer, you should not have a hard time implementing cross-attention and a full encoder-decoder Transformer to train on arbitrary sequence-to-sequence tasks.\n",
    "We will explore these topics in the next weeks.\n",
    "That said, here is some further reading material should you want to dive deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff92e-3808-41b0-a4fe-c3746b6acfdc",
   "metadata": {},
   "source": [
    "### 4.1 Papers & Blogs\n",
    "\n",
    "- [Attention Is All You Need, Vaswani et al. 2017 (Original Transformer paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [Language Models are Unsupervised Multitask Learners, Radford et al. 2018 (GPT-2 paper)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners, Brown et al. 2020 (GPT-3 paper)](https://arxiv.org/abs/2005.14165)\n",
    "- [PaLM: Scaling Language Modeling with Pathways, Chrowdherry et al. 2022](https://arxiv.org/abs/2204.02311)\n",
    "- [LLaMA: Open and Efficient Foundation Language Models, Touvron et al. 2023](https://arxiv.org/abs/2302.13971)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models, Touvron et al. 2023](https://arxiv.org/abs/2307.09288)\n",
    "- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)\n",
    "- [Scaling Laws for Neural Language Models, Kaplan et al. 2020](https://arxiv.org/abs/2001.08361)\n",
    "- [Training Compute-Optimal Large Language Models, Hoffmann et al. (Chinchilla)](https://arxiv.org/abs/2203.15556)\n",
    "- [Pixel Recurrent Neural Networks, Van den Oord et al. 2016 (PixelCNN)](https://arxiv.org/abs/1601.06759)\n",
    "- [PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications, Salimans et al. 2017](https://arxiv.org/abs/1701.05517)\n",
    "- [Zero-Shot Text-to-Image Generation, Ramesh et al. 2021 (DALL-E 1)](https://arxiv.org/abs/2102.12092)\n",
    "- [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation, Sun et al. 2024 (Image generation with simple autoregressive models)](https://arxiv.org/abs/2406.06525)\n",
    "- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al. 2020 (Vision Transformer paper)](https://arxiv.org/abs/2010.11929)\n",
    "- [Transformer Circuits Thread](https://transformer-circuits.pub/)\n",
    "- [The Transformer Family Version 2.0, Lilian Weng 2023](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [The Illustrated Transformer, Jay Alammar 2018](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ec4f0-9057-4a8c-ac92-dc9603c5012c",
   "metadata": {},
   "source": [
    "### 4.2 Code bases\n",
    "\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Llama model inference code](https://github.com/meta-llama/llama-models)\n",
    "- [LlamaGen](https://github.com/FoundationVision/LlamaGen)\n",
    "- [torchtitan](https://github.com/pytorch/torchtitan)\n",
    "- [lingua](https://github.com/facebookresearch/lingua)\n",
    "\n",
    "You might find that, in many ways, these repos are not that much different from what we implemented here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5682-f0d7-415c-9c7c-6aae68b0fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano4M kernel (nanofm)",
   "language": "python",
   "name": "nanofm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
