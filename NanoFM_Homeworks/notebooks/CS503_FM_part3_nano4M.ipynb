{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS-503 Foundation Models: Part 3 - nano4M\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this second part is to familiarize yourself with the following topics:\n",
    "- Cross-attention\n",
    "- Encoder-decoder Transformer (e.g. T5, 4M, ...) models\n",
    "- Multimodal masking schemes\n",
    "- Masked modelling on text, images, and other modalities\n",
    "- Multimodal masked inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with multimodal masked image models, like [4M](https://4m.epfl.ch/).\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the accompanying codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed (including the `assets` directory), as well as `nanofm/models/fourm.py` and `nanofm/modeling/transformer_layers.py`.\n",
    "- The notebooks are individual homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup\n",
    "\n",
    "1. Please follow the instructions in the [README.md](../README.md) file to set up the environment.\n",
    "\n",
    "2. Please download the CLEVR dataset from [this link](https://drive.google.com/file/d/1QRFqoGKMFYlgxYfPr9O7PeOadzXAbJI8/view?usp=sharing), and extract it to the root directory of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "# Imports for Cosmos tokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.multimodal.simple_multimodal_dataset import SimpleMultimodalDataset\n",
    "from nanofm.data.multimodal.masking import SimpleMultimodalMasking\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d3bb3-0c87-4f64-8b7d-b1d9fe03d48a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 Introduction to nano4M\n",
    "\n",
    "In this exercise, we will implement a simplified *multimodal* masked generative model, inspired by [4M](https://4m.epfl.ch/). The aim of this is to create a minimal working example of training an encoder-decoder Transformer on image-like and sequence-like modalities, resulting in an any-to-any model that can predict any of the training modalities from any subset of other modalities. 4M is an encoder-decoder architecture that is trained through a multimodal masked modeling objective on top of discrete multimodal tokens. This means that all modalities, whether they are image-like or sequence-like, are unified into a discrete representation space. The 4M objective consists of predicting one random subset of all tokens from another random subset of all tokens. At inference time, 4M is an any-to-any multimodal generative model that can predict through iterative unmasking, similar to MaskGIT from part 2, but now multimodal.\n",
    "\n",
    "### 2.1 Data and modalities\n",
    "We will train nano4M on a simple multimodal version of [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/), covering the following modalities/tasks:\n",
    "\n",
    "- **RGB**: RGB image modality.\n",
    "- **Depth**: The distance to the surface for every pixel. Darker = closer, brighter = further away.\n",
    "- **Surface normals**: The orientation of the surface for every pixel, represented as a color. Purple-ish = pointing upwards, green-ish = pointing left, yellow-ish = pointing right, ...\n",
    "- **Scene descriptions**: A textual description of the position, shape, color, and material for every object in the scene.\n",
    "\n",
    "**Example RGB, depth and surface normals**:\n",
    "\n",
    "![Multimodal CLEVR example](./assets/multimodal_clevr_example.png)\n",
    "\n",
    "**Example scene description**: `Object 1 - Position: x=77 y=51 Shape: cube Color: blue Material: metal. Object 2 - Position: x=57 y=34 Shape: sphere Color: cyan Material: rubber. Object 3 - Position: x=40 y=43 Shape: sphere Color: purple Material: metal. Object 4 - Position: x=46 y=60 Shape: cylinder Color: gray Material: rubber. Object 5 - Position: x=67 y=40 Shape: cylinder Color: brown Material: rubber.`\n",
    "\n",
    "Depending on the number of objects the length of this scene description can change. In our simplified version of 4M we will model sequence modalities through random masking (exactly like the image modalities), rather than span-masking. That means, we will also predict the `[PAD]` tokens up to the maximum sequence length such that the model can decide how long the predicted sequence should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db17df6-8e3c-4606-b771-2c3d73fecdd9",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization\n",
    "\n",
    "In our example we are dealing with two kinds of modalities: \n",
    "\n",
    "**1. Text-like modalities like the scene description**:\n",
    "\n",
    "Many modalities like captions (or in our case scene descriptions), bounding boxes, metadata, poses, etc. can simply be described as a text sequence and turned into discrete tokens using an off-the-shelf text tokenizer.\n",
    "Like in the previous notebooks, we will simply use the GPT-2 tokenizer for this purpose.\n",
    "\n",
    "**2. Image-like modalities like RGB, depth and surface normals**:\n",
    "\n",
    "For image-like modalities we are dealing with two issues. \n",
    "The first is that they are usually high-dimensional. For example, a 3-channel 256x256 pixel image would consist of nearly 200'000 tokens if flattened naively, with each token representing a single color of a single pixel. The longer the sequences, the more compute we require to model the data, and in this simple variant, each token contains only a minute amount of information (only one byte).\n",
    "The second issue is that to train an autoregressive model we usually require our data to be discrete-valued in order to train it with the cross-entropy loss (although recently that requirement has been relaxed.) How can we turn images and other image-like modalities into discrete representations?\n",
    "\n",
    "Like 4M, we rely on image tokenizers to lossily compress images, depth maps, and surface normal maps into more compact discrete token grids. 4M used modality-specific tokenizers, but for simplicity we will render modalities like depth and surface normals as RGB images and use an off-the-shelf RGB tokenizer to encode them all. To that end, we use a [Cosmos](https://github.com/nvidia/cosmos) image tokenizer, and load it with the following helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df89d0-bd7c-4df5-9c2e-f1222f898d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\n",
    "    repo_id='nvidia/Cosmos-0.1-Tokenizer-DI16x16', \n",
    "    local_dir='/tmp/nvidia/Cosmos-0.1-Tokenizer-DI16x16' # If you cannot access this path, feel free to replace it with any other directory you have permission to write to; it is only used as a cache location for downloading the tokenizer.\n",
    ")\n",
    "image_tokenizer = ImageTokenizer(\n",
    "    checkpoint_enc='/tmp/nvidia/Cosmos-0.1-Tokenizer-DI16x16/encoder.jit', \n",
    "    checkpoint_dec='/tmp/nvidia/Cosmos-0.1-Tokenizer-DI16x16/decoder.jit',\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529d871-5a31-47d3-a236-69e336436517",
   "metadata": {},
   "source": [
    "Let us load an example image and encode it with the Cosmos tokenizer. \n",
    "The 3-channel 256x256 image gets turned into a 16x16 grid of discrete tokens, which can take values in [0, 63999]. This step heavily compresses the image, down to 512 bytes (16\\*16 tokens, each 2 bytes to model the 64000-sized vocabulary)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d8bcd-dbc9-4be2-b462-4a81b13e41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('./notebooks/assets/example_image.png').resize((256,256)).convert('RGB')\n",
    "img_tensor = TF.to_tensor(img).to(device).unsqueeze(0) * 2 - 1\n",
    "print(img_tensor.shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56946e6-1fc8-4b9f-89fb-fe1048e71445",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, _ = image_tokenizer.encode(img_tensor)\n",
    "print(tokens.shape)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef69dc1-ccbf-4d29-a4fd-c36480986fd9",
   "metadata": {},
   "source": [
    "Let's reconstruct using the tokenizer's decoder and plot the original and reconstructed image side-by-side. As you can observe, the tokenizer reconstruction does not preserve pixel-perfect details, but gives us a more semantic encoding of the image. For example, the brush strokes are not exactly the same, but they are in a similar style. Or the bird's eye is still very much recognizable as an eye, but abstracts away more nuanced details about it. Generally the reconstruction fidelity can be improved by increasing the input image size (e.g. a 512x512 pixel image gets mapped into 32x32 tokens here), changing the tokenizer to use a smaller spatial compression ratio (e.g. from 16x reduction of each side to only an 8x reduction), or by increasing the vocabulary size (e.g. increasing it beyond 64000). The first two techniques result in larger sequence lengths, while the third increases the size of the input embedding and to_logit projection layers. In our examples we will stick with a decent middle-ground - encoding 256x256 pixel images into 16x16 tokens of vocabulary size 64000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ab938-ed11-4f9b-81e1-a51ad358785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = image_tokenizer.decode(tokens).float().clamp(-1,1)\n",
    "reconst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e4c18-2dc7-41bb-8f60-fccfd76bd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Original RGB')\n",
    "ax[1].imshow(reconst[0].cpu().permute(1,2,0) / 2 + 0.5)\n",
    "ax[1].set_title('Cosmos tokenizer reconstruction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643652eb-543c-4cff-b763-0beec6eb01f3",
   "metadata": {},
   "source": [
    "### 2.3 Multimodal masked dataset\n",
    "\n",
    "As mentioned above, we will train on a multimodal version of CLEVR. As a general rule when training generative models on top of tokens, we want to spend as much of the GPU's FLOPS on the actual model training, and not on encoding the various modalities. While the tokenizer overhead can be manageable with a single modality and a small tokenizer encoder, it can quickly get out of hand with multiple modalities. For this reason, we provide a pre-tokenized dataset, which also has the benefit of being significantly more compact than the original images. \n",
    "\n",
    "In `nanofm/data/multimodal/simple_multimodal_dataset.py` we define a simple multimodal dataset that loads these tokens from the disk. Feel free to take a look. This dataloader merely loads the tokens, but since we are training a model with a multimodal masking scheme, we also need to perform that masking logic somewhere. We could perform it inside the actual model's forward pass, but it is cleaner and easier to perform that logic within a transform that is applied to each sample individually. Please see `nanofm/data/multimodal/masking.py` for the masking logic. We will discuss it more in the following on some concrete examples.\n",
    "\n",
    "For that, let us first define some helper functions for plotting samples form the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bc442-52f5-4149-8255-cea5387de91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_image(token_ids, image_tokenizer, to_pil=False):\n",
    "    n_tokens = token_ids.numel()\n",
    "    side = int(math.sqrt(n_tokens))\n",
    "    token_ids = token_ids.reshape(1,side,side).to(device)\n",
    "    reconst = image_tokenizer.decode(token_ids)\n",
    "    reconst = (reconst[0].clamp(-1,1).float().cpu() + 1) / 2\n",
    "    if to_pil:\n",
    "        reconst = TF.to_pil_image(reconst)\n",
    "    return reconst\n",
    "\n",
    "def get_modality_with_mask(data_dict, mod_name, mask_type=None):\n",
    "    tokens = data_dict['unmasked_data_dict'][mod_name].clone()\n",
    "    L = tokens.shape[0]\n",
    "    if mask_type == 'input':\n",
    "        modality_mask = data_dict['enc_modalities'] == modalities.index(mod_name)\n",
    "        modality_mask = modality_mask & data_dict['enc_pad_mask']\n",
    "        modality_indices = data_dict['enc_positions'][modality_mask]\n",
    "    elif mask_type == 'target':\n",
    "        modality_mask = data_dict['dec_modalities'] == modalities.index(mod_name)\n",
    "        modality_mask = modality_mask & data_dict['dec_pad_mask']\n",
    "        modality_indices = data_dict['dec_positions'][modality_mask]\n",
    "    elif mask_type is None:\n",
    "        modality_indices = torch.arange(L)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    mask = torch.zeros(L, dtype=torch.bool)\n",
    "    mask[modality_indices] = True # True = input, False = masked out\n",
    "    return tokens, mask\n",
    "\n",
    "def get_masked_image(img, mask):\n",
    "    mask = mask.reshape(1,1,16,16).float()\n",
    "    mask = F.interpolate(mask, (256,256), mode='nearest')[0,0].bool()\n",
    "    img[:,~mask] = 0\n",
    "    img = img.permute(1,2,0)\n",
    "    return img\n",
    "\n",
    "def plot_data_dict_with_mask(data_dict, mask_type=None):\n",
    "    # Scene description\n",
    "    tokens, mask = get_modality_with_mask(data_dict, 'scene_desc', mask_type=mask_type)\n",
    "    n_scene_desc = mask.sum().item()\n",
    "    tokens[~mask] = 62 # Simply plot masked out regions as underscores\n",
    "    print(f'Scene description: {dataset.text_tokenizer.decode(tokens)}\\n\\n')\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1, 3), axes_pad=0.1)\n",
    "    \n",
    "    # RGB\n",
    "    tokens, mask = get_modality_with_mask(data_dict, 'tok_rgb@256', mask_type=mask_type)\n",
    "    n_rgb = mask.sum().item()\n",
    "    img = get_masked_image(token_ids_to_image(tokens, image_tokenizer), mask)\n",
    "    grid[0].imshow(img)\n",
    "    grid[0].set_title('RGB')\n",
    "    \n",
    "    # Depth\n",
    "    tokens, mask = get_modality_with_mask(data_dict, 'tok_depth@256', mask_type=mask_type)\n",
    "    n_depth = mask.sum().item()\n",
    "    img = get_masked_image(token_ids_to_image(tokens, image_tokenizer), mask)\n",
    "    grid[1].imshow(img)\n",
    "    grid[1].set_title('Depth')\n",
    "    \n",
    "    # Normals\n",
    "    tokens, mask = get_modality_with_mask(data_dict, 'tok_normal@256', mask_type=mask_type)\n",
    "    n_normal = mask.sum().item()\n",
    "    img = get_masked_image(token_ids_to_image(tokens, image_tokenizer), mask)\n",
    "    grid[2].imshow(img)\n",
    "    grid[2].set_title('Normals')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if mask_type is not None:\n",
    "        print()\n",
    "        print(f'Num. scene_desc tokens: {n_scene_desc}')\n",
    "        print(f'Num. tok_rgb@256 tokens: {n_scene_desc}')\n",
    "        print(f'Num. tok_depth@256 tokens: {n_depth}')\n",
    "        print(f'Num. tok_normal@256 tokens: {n_normal}')\n",
    "        print('-'*32)\n",
    "        total_tokens = n_scene_desc + n_rgb + n_depth + n_normal\n",
    "        print(f'Total tokens: {total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f1278-d954-4c13-9010-c27ab7cfd24d",
   "metadata": {},
   "source": [
    "Furthermore, let us define the mask transform and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6980481-0095-4bb0-bc25-e98e3cc7ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['tok_rgb@256', 'tok_depth@256', 'tok_normal@256', 'scene_desc']\n",
    "vocab_sizes = [64000, 64000, 64000, 50304]\n",
    "max_seq_lens = [256, 256, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ac979-dae5-424f-aad5-79e3f5a97de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_transform = SimpleMultimodalMasking(\n",
    "    modalities=modalities,\n",
    "    vocab_sizes=vocab_sizes,\n",
    "    max_seq_lens=max_seq_lens,\n",
    "    input_alphas=[1.0, 1.0, 1.0, 1.0],\n",
    "    target_alphas=[1.0, 1.0, 1.0, 1.0],\n",
    "    input_tokens_range=(1,128),\n",
    "    target_tokens_range=(1,128),\n",
    "    overlap_vocab=True,\n",
    "    overlap_posembs=True,\n",
    "    include_unmasked_data_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca67c9-e52b-4baa-b535-41cfd144674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = SimpleMultimodalDataset(\n",
    "    root_dir='clevr', # REPLACE WITH YOUR CLEVR DATASET PATH\n",
    "    split='train',\n",
    "    modalities=modalities,\n",
    "    sample_from_k_augmentations=10,\n",
    "    text_tokenizer_path='gpt2',\n",
    "    text_max_length=256,\n",
    "    transforms=mask_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55493fb-1392-45cd-b08d-260b94b1eca5",
   "metadata": {},
   "source": [
    "Now that we defined the dataset and masking function, let's look at an example. The training objective of 4M is to predict *one random subset of tokens of all modalities from another random subset*. These subsets don't overlap, i.e. no target token can be in the input too (otherwise the model may simply copy the information).\n",
    "\n",
    "We first show the original data for the four modalities, without any masks. The second row below shows the input to the encoder, and the third row shows the target, i.e. what we train the model to predict given the input.\n",
    "\n",
    "To create the masks, we first randomly sample the number of input and target tokens for each modality, given a certain input and target token budget. In other words, let's say we want to always have 128 tokens in the input and 128 tokens in the output, such that we can batch multiple random samples efficiently. If we define that number, the first task is to randomly assign how many tokens we will randomly sample from each modality to fulfill that 128 token input/output budget. We do that by sampling from a Dirichlet distribution (see https://multimae.epfl.ch/). Once we have the number of input and target tokens for each modality, we simply draw tokens uniformly at random from the modalities according to those budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade00b0-e10b-488a-8866-5220ddba4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dataset[0]\n",
    "\n",
    "print('#'*10 + ' Original non-masked data ' + '#'*10 + '\\n')\n",
    "plot_data_dict_with_mask(data_dict, mask_type=None)\n",
    "\n",
    "print('\\n\\n' + '#'*10 + ' Inputs ' + '#'*10 + '\\n')\n",
    "plot_data_dict_with_mask(data_dict, mask_type='input')\n",
    "\n",
    "print('\\n\\n' + '#'*10 + ' Targets ' + '#'*10 + '\\n')\n",
    "plot_data_dict_with_mask(data_dict, mask_type='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36597e5f-9c14-46b6-85b2-e0fdf531047e",
   "metadata": {},
   "source": [
    "Feel free to load a few examples, inspect the original un-masked data_dict, and try to understand what everything means. Please also check out https://multimae.epfl.ch/ and https://4m.epfl.ch/ for more details on the multimodal training objective.\n",
    "\n",
    "Let's dig deeper into how we provide that information to the model. From the perspective of the encoder, we need to pass three types of information:\n",
    "\n",
    "1. **What** content or information does a particular token contain? This is simply the token ID we get from the tokenizer, and inside nano4M we learn an Embedding layer to map those indices to vectors.\n",
    "2. **Where** is that token from? I.e. what's the token's x,y location in the 2D token grid, or the position in the 1D sequence of text tokens? We simplify this by just considering the token position index of flattened sequences. All image-like modalities are 16x16 tokens flattened to 256 tokens, and the scene description is also at most 256 tokens. Inside nano4M, we simply use sin-cos positional embeddings to specify that position to the model.\n",
    "3. **What** modality is that token? We simplify the design of nano4M by using a single unified vocabulary for all modalities. That means that the i-th vocabulary entry can be reused for all modalities, and have a different meaning for each. To disambiguate if the i-th token came from e.g. normals or the scene description, we simply learn a modalty embedding in nano4M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ef583-f4f1-482f-8d07-ad7e6d2fd5c2",
   "metadata": {},
   "source": [
    "First, let's plot the modality indices for the random subset of tokens that gets encoded. Here the indices correspond to the above modalities variable `['tok_rgb@256', 'tok_depth@256', 'tok_normal@256', 'scene_desc']`, i.e. index 0 is RGB, 1 is depth, and so on.\n",
    "\n",
    "Note: Notice that the last few tokens are 0's too. These are place-holder indices for padding tokens, i.e. they are not used. Later we will look at the encoder and decoder padding masks that are used to ignore these positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c471175-74a4-4d08-bb3e-35a77db7eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['enc_modalities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2d4e1-c6ee-41ea-862d-d38758def7d9",
   "metadata": {},
   "source": [
    "At these positions we pass the token information / the actual content describing *what* the input contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861270b6-c779-49a4-a466-d1bc90905e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['enc_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92350774-9b2f-4588-9569-4f38150674cd",
   "metadata": {},
   "source": [
    "And as described above, we need to let the model know where these tokens came from in each sequence, such that we can look up the corresponding positional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e3393-c72f-4cf7-8176-ad12d2a3f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['enc_positions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865a2da-8061-4cca-8caf-9255f7fe995f",
   "metadata": {},
   "source": [
    "As a side-note, we also pass masks that indicate padding tokens if the encoder tokens don't fill up the token budget. (We do the same for the decoder.) For both, True = a token is used, False = a token is padding and should not be attended to (and no loss should be computed on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3e2e0-4277-4760-8807-d9ed632780b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['enc_pad_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545b1fd-0482-4d01-ad59-6bb447b1a4da",
   "metadata": {},
   "source": [
    "From the perspective of the decoder, we need to tell it where we want to predict (or read out) a token, and for which modality. This is basically the same as in the encoder, but this time we don't pass the token information into the decoder (since that's what we want to predict!) Instead, we need it as the target, i.e. for computing the loss. See the following tensors.\n",
    "\n",
    "Note: For the `'dec_tokens'` tensor, we use `-100` as the padding index. We use this to signify which tokens to ignore in the cross-entropy loss calculation. For the other tensors we use `0`, since it doesn't matter much what we put when these positions are ignored in the attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3367cc-e7a9-4632-bf28-7e21b814c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['dec_modalities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8ceb7-c1ec-48b7-a7ca-a0431d34824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['dec_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f6b22-6cb8-4bed-97b9-3a8307167eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['dec_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d10ff-3b85-4bba-b17e-52aaaff9395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['dec_pad_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae1709-b07f-48ad-bea2-d4e17088aceb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Building nano4M\n",
    "\n",
    "To implement nano4M, we ask you to complete the subsections below by directly filling in the missing lines in the code base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955107d-3452-40ae-afc4-0b7ecdf76728",
   "metadata": {},
   "source": [
    "### 3.1 (Masked) cross-attention layer (20 points)\n",
    "\n",
    "First, let's implement an (optionally masked) multi-headed cross-attention layer in `nanofm.modeling.transformer_layers.CrossAttention`.\n",
    "We will need this for the Transformer decoder to attend to the encoded tokens using cross-attention.\n",
    "\n",
    "This layer is very similar to the already implemented self-attention layer, with the difference that the queries are produced by the input sequence $X \\in \\mathcal{R}^{N \\times D}$ ($N$ tokens of dimension $D$), while the keys and values are produced by the context $C \\in \\mathcal{R}^{M \\times D}$ ($M$ tokens of dimension $D$).\n",
    "The queries $Q$ are linear projections of $X$, while the keys $K$ and the values $V$ are both linear projections of $C$.\n",
    "\n",
    "$$ Q(X) = X W_Q^T $$\n",
    "$$ K(C) = C W_K^T $$\n",
    "$$ V(C) = C W_V^T $$\n",
    "\n",
    "\n",
    "The scaled dot-product attention then given these queries, keys and values is the same:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}_\\text{row} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "The scaling factor $d_k$ is the dimensionality of the keys $K$, i.e. `dim // num_heads`.\n",
    "\n",
    "The attention is performed on `num_heads` heads in parallel (don't use a for loop) in `head_dim = dim // num_heads`-dimensional subspaces and the results are concatenated along the feature dimension.\n",
    "\n",
    "In addition, we want to enable masking of the attention matrix, e.g. for preventing the cross-attention to attend to encoder padding tokens.\n",
    "For this, the forward function takes an additional `mask` argument, specifying where to zero-out the attention matrix.\n",
    "In practice, this is implemented by replacing the values of the attention matrix (pre softmax) to minus infinity wherever we don't want any attention flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049abef2-997d-4ad1-9d7b-bc471e189fbb",
   "metadata": {},
   "source": [
    "### 3.2 Transformer decoder block (10 points)\n",
    "\n",
    "Next, implement a Transformer decoder block in `nanofm.modeling.transformer_layers.DecoderBlock`. It is defined as:\n",
    "\n",
    "$$ X_a = X + \\text{Self-Attention}(\\text{LN}(X)) $$\n",
    "$$ X_b = X_a + \\text{Cross-Attention}(\\text{LN}(X_a), \\text{LN}(C)) $$\n",
    "$$ X_c = X_b + \\text{MLP}(\\text{LN}(X_b)) $$\n",
    "\n",
    "Here, $\\text{LN}$ denotes (four separate) LayerNorm layers.\n",
    "\n",
    "Don't forget to pass the optional self-attention (sa) mask to the self-attention layer, and cross-attention (xa) mask to the cross-attention layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088d5ab-5b08-44f7-a92c-afaf6cbac661",
   "metadata": {},
   "source": [
    "### 3.3 Assembling the blocks into a Transformer decoder tunk (10 points)\n",
    "\n",
    "Now we have all the building blocks to create a Transformer decoder trunk! \n",
    "\n",
    "In `nanofm.modeling.transformer_layers.TransformerDecoderTrunk`, create an `torch.nn.ModuleList` containing multiple Transformer decoder blocks, and in the forward pass call them one after another.\n",
    "Again, make sure to pass the masks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca4ef4-474f-43f7-87b4-f01012c9c2b5",
   "metadata": {},
   "source": [
    "### 3.4 Initialize nano4M (10 points)\n",
    "\n",
    "Now we have all the building blocks to construct an encoder-decoder Transformer and make it multimodal. We will implement it in `nanofm.models.fourm.FourM`.\n",
    "Please initialize the following modules in the constructor:\n",
    "1. The input tokens (from `data_dict['enc_tokens']`) are embedded with an `nn.Embedding` layer. Initialize `self.enc_tok_emb` accordingly, taking into account the vocabulary size `self.vocab_size`.\n",
    "2. Different from the previous notebooks, we use fixed 1D sine-cosine positional embeddings, which are initialized in `self.pos_emb`. We already provide that step for you.\n",
    "3. Since we use a shared vocabulary, we need to tell the encoder from which modality each token stems. For that, we learn another `nn.Embedding` layer. Initialize `self.enc_mod_emb` accordingly, taking into account the number of modalities `self.num_modalities`.\n",
    "4. The sum of the token embedding, positional embedding, and modality embedding gets passed to a standard Transformer encoder trunk. Initialize `self.encoder` with the Transformer trunk from weeks 1 and 2.\n",
    "5. After encoding these tokens, we pass the encoder output through a LayerNorm and a projection from the encoder's dimension to the decoder's (which are the same). Initialize `self.enc_norm` and `self.dec_context_proj`. This concludes the setup for the encoder.\n",
    "6. For the decoder we need to specify where we want to predict each token (using the same fixed positional embeddings mentioned before), and for which modality. For the latter, let's initialize another `nn.Embedding` layer as `self.dec_mod_emb`, similar to `self.enc_mod_emb`.\n",
    "7. The sum of the position and modality embeddings to decode gets passed through a Transformer decoder layer. Using the TransformerDecoderTrunk you implemented, initialize `self.decoder`.\n",
    "8. Finally we project the decoder output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size `self.vocab_size` (as a one-hot vector per token). Initialize `self.dec_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2086b-0be9-46e3-b81a-f4f2b0264c7b",
   "metadata": {},
   "source": [
    "### 3.5 nano4M encoder forward (10 points)\n",
    "\n",
    "Next, implement the `forward_encoder` function:\n",
    "1. The input to the encoder is the sum of the encoder input token embeddings, corresponding position embeddings, and corresponding modality embeddings.\n",
    "2. Pass this tensor through the encoder, making sure to pass the optional self-attention mask.\n",
    "3. Pass the encoder output through the encoder norm layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cdb80-1872-4c7c-bf25-7ec614010b39",
   "metadata": {},
   "source": [
    "### 3.6 nano4M decoder forward (10 points)\n",
    "\n",
    "Next, implement the `forward_decoder` function:\n",
    "1. The input to the decoder is the sum of the target position embeddings, and corresponding modality embeddings.\n",
    "2. Pass this tensor through the decoder, making sure to pass the optional self-attention and cross-attention masks.\n",
    "3. Pass the decoder output through the decoder norm layer.\n",
    "\n",
    "In `forward_model`, project that decoder output to the vocabulary logits using `self.to_logits`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b72d77-5a51-41fa-b850-f1a654685af4",
   "metadata": {},
   "source": [
    "### 3.7 Generation (10 points)\n",
    "\n",
    "4M defines two decoding schemes to generate the desired targets: \n",
    "\n",
    "#### MaskGIT-style decoding\n",
    "This decoding scheme is pretty much the same as the one you implemented in part 2 for MaskGIT, with the difference that we now use an encoder-decoder Transformer instead of an encoder-only Transformer. That means the encoder exclusively processes \"visible/input\" tokens, while the decoder is in charge of processing the mask tokens to predict the missing regions. Like with MaskGIT, in this scheme we always decode all remaining masked-out tokens and pick the $k$ most confident ones to sample from.\n",
    "\n",
    "#### Random Order Auto-Regressive (ROAR) decoding\n",
    "With 4M, we can decode tokens in a slightly different way. Rather than always decoding all masked-out tokens and then sampling the top-k most confident ones, we can instead randomly sample k token positions to decode and only process those in the decoder. This simplifies some of the considerations on *how* we even decide which tokens are the most confident ones in MaskGIT. This decoding scheme has recently been adopted in [MAR](https://arxiv.org/abs/2406.11838).\n",
    "\n",
    "In this task, we ask you to implement the ROAR decoding scheme in `generate_one_modality_roar`, focusing on generating a single modality given any input. Specifically it works in the following way:\n",
    "1. The function expects the input tokens, their positions, and their modality. Given the desired number of decoding steps and target modality, it performs the iterative unmasking and returns the predicted tokens, as well as the original inputs concatenated with the predicted tokens/positions/modalities (to be used as input when generating other modalities in a chained manner).\n",
    "2. Given the desired number of decoding steps, it computes the number of tokens $k$ to predict at each step (i.e. the schedule).\n",
    "3. During each decoding step we select a random set of $k$ positions to decode, which are simply position indices between 0 and the maximum number of tokens (255 for each modality). These desired target positions to predict, alongside the corresponding target modality indices, are fed through the decoder to get the logits for those $k$ tokens. \n",
    "4. We then simply sample the token indices from these tokens. To prepare the input for the next round, we simply concatenate the predicted tokens with the encoder input tokens, the predicted positions with the encoder input positions, and the predicted modality ids with the input modality ids. \n",
    "5. After all the generation steps are done, we select the predicted subset of tokens, unshuffle it and return it.\n",
    "6. With the generated sequence so far, we can then simply input that into the same function to generate the next modalities in a chained manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2a3f8-2c2b-4dd3-b50e-19785b9af32e",
   "metadata": {},
   "source": [
    "### 3.8 Training the model\n",
    "\n",
    "We defined a training config for you in: `nano4M_sol/cfgs/nano4M/multiclevr_d6-6w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "This config is expected to be run on 4 V100 GPUs. That said, on SCITAS most nodes have 2 V100s, not 4. For that reason, we ask you to start the training jobs with the prepared multinode script (`submit_job_multi_node_scitas.sh`) in the root directory. \n",
    "(Just for reference, on a single 4xV100 node, you could start the training like: `OMP_NUM_THREADS=1 torchrun --nproc_per_node=4 run_training.py --config cfgs/nano4M/multiclevr_d6-6w512.yaml`. This command won't work for multi-node training; you should use the aforementioned script.)\n",
    "\n",
    "Training on 4 V100s should take around 5 hours. You should reach a final validation loss around 3.5, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nano4M_clevr.png\" alt=\"nano4M CLEVR loss curves\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297989d-8159-4428-9f5e-effffbc006a6",
   "metadata": {},
   "source": [
    "### 3.9 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "[Note] Your screenshot must clearly show your Weights & Biases (W&B) account (username/entity), usually visible in the top-right corner of the page.\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nano4M CLEVR loss curves\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb122a42-10dd-4f4b-ab34-310595f9f542",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4 Inference\n",
    "\n",
    "Let's load the trained model and check that we can perform any-to-any prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe52fb-13e7-47da-b82b-cefa8d194f56",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e91e72-c475-4e2a-9a79-67bf2b326e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './outputs/nano4M/multiclevr_d6-6w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7f42f-b094-47d5-b7ab-e814e47b9fb7",
   "metadata": {},
   "source": [
    "To test the model, let's load some (unmasked) samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07241b-3a1b-423e-8c35-2978f1f56345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleMultimodalDataset(\n",
    "    root_dir='/work/com-304/datasets/clevr_com_304/',\n",
    "    split='val',\n",
    "    modalities=modalities,\n",
    "    sample_from_k_augmentations=1,\n",
    "    text_tokenizer_path='gpt2',\n",
    "    text_max_length=256,\n",
    "    transforms=None, # No transforms\n",
    ")\n",
    "\n",
    "def construct_input_from_sample(dataset, idx, input_modality):\n",
    "    input_tensor = dataset[idx][input_modality]\n",
    "    n_tokens_input = input_tensor.shape[0]\n",
    "    enc_input_tokens = input_tensor.unsqueeze(0).to(device)\n",
    "    enc_input_positions = torch.arange(n_tokens_input, device=device).unsqueeze(0)\n",
    "    enc_input_modalities = modalities.index(input_modality) * torch.ones(1, n_tokens_input, device=device, dtype=torch.long)\n",
    "    return enc_input_tokens, enc_input_positions, enc_input_modalities\n",
    "\n",
    "def show_modality(tokens, modality):\n",
    "    if modality == 'scene_desc':\n",
    "        print(dataset.text_tokenizer.decode(tokens[0]))\n",
    "    else:\n",
    "        token_ids_to_image(tokens, image_tokenizer, to_pil=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d842f-317a-4cfd-922b-e7ee60fdc513",
   "metadata": {},
   "source": [
    "#### Example: RGB -> Depth -> Normals -> Scene description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c5966-dea5-45b6-8e1e-eadb82a8cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_modality = 'tok_rgb@256'\n",
    "sample_idx = 0\n",
    "\n",
    "x_tokens, x_positions, x_modalities = construct_input_from_sample(dataset, idx=sample_idx, input_modality=input_modality)\n",
    "show_modality(x_tokens, input_modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa873e-0e0a-414d-8ba4-7a65d3630b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'tok_depth@256'\n",
    "num_steps, temp, top_p, top_k = 8, 0.001, 0.0, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345ec9f-fba7-4b8e-9831-17903fab27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'tok_normal@256'\n",
    "num_steps, temp, top_p, top_k = 8, 0.001, 0.0, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a97420-a78f-4529-9963-6a0c12df1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'scene_desc'\n",
    "num_steps, temp, top_p, top_k = 128, 0.7, 0.9, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc301fe-6fe1-4f60-b2c1-6d8c2064df31",
   "metadata": {},
   "source": [
    "#### Example: Scene description -> Normals -> Depth -> RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7857723-916f-45b1-99b5-8d7617c57e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_modality = 'scene_desc'\n",
    "sample_idx = 0\n",
    "\n",
    "x_tokens, x_positions, x_modalities = construct_input_from_sample(dataset, idx=sample_idx, input_modality=input_modality)\n",
    "show_modality(x_tokens, input_modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa249223-3cba-4099-82d7-2900e450570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'tok_normal@256'\n",
    "num_steps, temp, top_p, top_k = 128, 0.7, 0.9, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fa195-78e6-4745-88fc-c2ef3053252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'tok_depth@256'\n",
    "num_steps, temp, top_p, top_k = 32, 0.7, 0.9, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567e493-b547-43b0-9943-c47cce90e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mod = 'tok_rgb@256'\n",
    "num_steps, temp, top_p, top_k = 64, 0.7, 0.9, 0.0\n",
    "\n",
    "pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "    x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "    num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    ")\n",
    "show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5680e-647d-42f9-999b-d026a9dc322c",
   "metadata": {},
   "source": [
    "### 4.2 Open-ended questions (10 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063195b-fb1e-4e66-9d8e-3975e5c9e25a",
   "metadata": {},
   "source": [
    "#### 4.2.1 Improved scene description -> RGB chain and hyperparameters\n",
    "\n",
    "Can you find a better set of generation hyperparameters (number of steps, temperature, top-k, top-p) and prediction chain to go from scene descriptions to RGB (using any or no intermediate modalities)? Show your results and observations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8177f54-50ff-494a-8199-41afefd0b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce68a84-dc47-4033-9d88-1eb63d012ee4",
   "metadata": {},
   "source": [
    "#### 4.2.2 Condition alignment\n",
    "\n",
    "The alignment between condition and prediction can be quite poor sometimes, especially with the scene descriptions. What techniques could we apply during train and/or inference time to improve alignment and the overall generation quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b78a42-bda1-4a59-b768-3303aec92896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bfec4-98c8-407b-bccd-8d37e058a401",
   "metadata": {},
   "source": [
    "#### 4.2.3 Influence of number of decoding steps\n",
    "\n",
    "How many decoding steps are needed to get decent results for different X → Y predictions? Test RGB → Normals, RGB → Scene Desc., and Scene Desc. → RGB. Why do some of these work well with very few steps and others not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6ed29-5c57-4a6b-97cb-20d990e2cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d1b50-f19b-487a-a74a-390e80b7244a",
   "metadata": {},
   "source": [
    "### 4.3 Train a nano4M model with captions (Bonus, 20 points)\n",
    "\n",
    "So far we trained nano4M with scene descriptions, but we also prepared synthetically generated captions for you in `/work/com-304/datasets/clevr_com_304/{train,val,test}/caption/`. \n",
    "Train a nano4M model with all the previous modalities (RGB, depth, normals, scene descriptions) plus the new `caption` modality. Show Caption → Any, and Any → Caption generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7e427-7b35-4b83-8dbf-978b4eb91e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371227e0-bbc7-452c-9ad9-e8ad45c1ae1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5 Further reading\n",
    "\n",
    "Here is some further reading material should you want to dive deeper on multimodal input & output models.\n",
    "- [4M: Massively Multimodal Masked Modeling](https://4m.epfl.ch/)\n",
    "- [MultiMAE: Multi-modal Multi-task Masked Autoencoders](https://multimae.epfl.ch/)\n",
    "- [Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks](https://arxiv.org/abs/2206.08916)\n",
    "- [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)\n",
    "- [UniDisc: Unified Multimodal Discrete Diffusion](https://unidisc.github.io/)\n",
    "- [CM3: A Causal Masked Multimodal Model of the Internet](https://arxiv.org/abs/2201.07520)\n",
    "- [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)\n",
    "- [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://arxiv.org/abs/2408.11039)\n",
    "- [LMFusion: Adapting Pretrained Language Models for Multimodal Generation](https://arxiv.org/abs/2412.15188)\n",
    "- [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811)\n",
    "- [Transframer: Arbitrary Frame Prediction with Generative Models](https://arxiv.org/abs/2203.09494)\n",
    "- [MetaMorph](https://tsb0601.github.io/metamorph/)\n",
    "- [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242)\n",
    "- [GiT: Towards Generalist Vision Transformer through Universal Language Interface](https://arxiv.org/abs/2403.09394)\n",
    "- [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/abs/2408.02657)\n",
    "- [Pix2seq: A Language Modeling Framework for Object Detection](https://arxiv.org/abs/2109.10852)\n",
    "- [A Unified Sequence Interface for Vision Tasks](https://arxiv.org/abs/2206.07669)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc381771-c446-4d82-9f79-dae74c801d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano4M kernel (nanofm)",
   "language": "python",
   "name": "nanofm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
