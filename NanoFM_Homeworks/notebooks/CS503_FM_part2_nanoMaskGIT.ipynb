{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS 503 Foundation Models: Part 2 - nanoMaskGIT\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this second part is to familiarize yourself with the following topics:\n",
    "- Bi-directional attention\n",
    "- Encoder-only Transformer (e.g. BERT, MaskGIT, ...) models\n",
    "- Basic masking schemes\n",
    "- Masked modelling on text and images\n",
    "- Masked inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with masked image models.\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the acompagning codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed, as well as `nanofm/models/maskgit.py`.\n",
    "- The notebooks are individual homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ae77a",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "Please follow the instructions in the [README.md](../README.md) file to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.vision.tokenized_mnist import create_tokenized_mnist_dataloader, detokenize_MNIST\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32547ef7-1554-4d80-80fa-bf0b3487dca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Training nanoMaskGIT on MNIST for image generation\n",
    "\n",
    "In this exercise, we will implement a simplified masked generative model, similar to [MaskGIT](https://masked-generative-image-transformer.github.io/). \n",
    "As with our nanoGPT implementation, we will train it on MNIST for image generation, but will also later explore using it for text generation on TinyStories!\n",
    "\n",
    "#### Masked modeling - Training objective\n",
    "\n",
    "In contrast to autoregressive models that are trained to predict the next token given the context so far, masked generative models like MaskGIT are trained to predict any (masked-out) token given any other (non-masked) subset of tokens.\n",
    "Consider the following example: \n",
    "\n",
    "**Original Sentence:**  \n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Masked Training Example (cloze):**  \n",
    "```\n",
    "\"The quick [MASK] fox jumps over the [MASK] dog.\"\n",
    "```\n",
    "\n",
    "**Goal:**  \n",
    "The model must predict:\n",
    "- `[MASK]` → \"brown\"\n",
    "- `[MASK]` → \"lazy\"\n",
    "\n",
    "By repeatedly training the model to predict these randomly masked tokens across a large dataset, MaskGIT learns how tokens fit contextually within sequences.\n",
    "\n",
    "\n",
    "#### Masked modeling - Inference\n",
    "\n",
    "\n",
    "By training a model with randomized masking ratios, we are able to use it to progressively \"unmask\" a fully masked initial sequence.\n",
    "At inference, the model starts with all tokens masked and progressively unmasks tokens in multiple steps. Let's show an inference example generating two (`k=2`) tokens at a time.\n",
    "\n",
    "**Initial Masked Sequence:**  \n",
    "```\n",
    "\"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\"\n",
    "```\n",
    "\n",
    "**Step-by-step Generation (k=2 tokens at a time):**  \n",
    "\n",
    "- **Step 1:**  \n",
    "```\n",
    "\"The [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] dog.\"\n",
    "```\n",
    "\n",
    "- **Step 2:** (unmask next 2 most confident tokens)  \n",
    "```\n",
    "\"The quick [MASK] [MASK] [MASK] [MASK] [MASK] lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 3:** (unmask next 2 tokens)  \n",
    "```\n",
    "\"The quick [MASK] fox [MASK] [MASK] [MASK] lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 4:** (unmask next 2 tokens)  \n",
    "```\n",
    "\"The quick brown fox jumps [MASK] the lazy dog.\"\n",
    "```\n",
    "\n",
    "- **Step 5 (Final):** (all tokens unmasked)  \n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "At each inference step, MaskGIT predicts all masked-out tokens simultaneously (in parallel), and, based on the predicted probabilites, selects the `k` (here `k=2`) most likely tokens. \n",
    "After deciding on which tokens to use, we sample a token index from the predicted probability distribution for each token, and add the tokens to the sequence. \n",
    "This, now slightly less masked, sequence is then used as the input for the next round, where again, the `k=2` most confident tokens are chosen of the remaining masked targets.\n",
    "\n",
    "A crucial difference of masked models to next-token prediction is that at each inference step we can freely choose the number of tokens `k` to simultaneously decode. \n",
    "Depending on the choice of `k`, this can speed up inference significantly, at little cost to generation performance. \n",
    "For example, see the comparison between raster-scan autoregressive, and masked generation below.\n",
    "Each frame of the gif is one generation step. Autoregressive generation predicts each token one-by-one, while masked models may predict them in parallel.\n",
    "\n",
    "![adsf](https://masked-generative-image-transformer.github.io/imgs/sampling.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ec3ac-cbc1-4ace-808f-a0ed9b88ae6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Overview and tasks\n",
    "\n",
    "To implement nanoMaskGIT, we ask you to complete the subsections below by directly filling in the missing lines in the code base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a0338-c83f-4b7f-b4bf-52e15babc394",
   "metadata": {},
   "source": [
    "#### 2.1.1 Initialize nanoMaskGIT (5 points)\n",
    "\n",
    "We will reuse the exact same Transformer layers and trunk built last week for the nanoGPT model, but this time we will use it to assemble a MaskGIT-like model in `nanofm.models.maskgit.MaskGIT`.\n",
    "It consists of a few operations executed in series. Initialize the following modules in the constructor:\n",
    "1. The discrete input tokens are embedded with an `nn.Embedding` layer. Initialize `self.input_embedding` accordingly, taking into account the vocabulary size.\n",
    "2. On top of that, we add learnable positional embeddings. Initialize `self.positional_embedding` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`max_seq_len`, `dim`).\n",
    "3. To indicate masked-out tokens and provide placeholders to write the targets, initialize `self.mask_token` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`dim`).\n",
    "4. This then gets passed to a Transformer trunk. Initialize `self.trunk` with the trunk you implemented last week.\n",
    "5. Finally we project the trunk output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size (as a one-hot vector per token). Initialize `self.out_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea932812-cd3e-4cd6-8d0d-b5ffddd38917",
   "metadata": {},
   "source": [
    "#### 2.1.2 Implement the forward function and loss (10 points)\n",
    "\n",
    "Next, let's implement the `forward_model` function:\n",
    "1. Pass the input tokens through the embedding. \n",
    "2. Given the `mask`, replace these embeddings by the learned `self.mask_token`, wherever `mask == True`.\n",
    "3. Add the positional embedding, pass it through the Transformer trunk, output normalization, and output projection.\n",
    "4. When calling the Transformer trunk, no attention mask needs to be specified. This model performs full self-attention between all masked and non-masked tokens.\n",
    "\n",
    "Finally, we need to compute the cross-entropy loss between the logits and the ground-truth targets. Please complete the `compute_ce_loss` function accordingly, and take into account the ignore_index token. We do not want to compute a loss on non-masked tokens that we pass as input; we only compute it on masked-out tokens that we predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab6ee5-36ad-4b03-b053-1188750696fc",
   "metadata": {},
   "source": [
    "#### 2.1.3 Implement random masking (15 points)\n",
    "\n",
    "As used in the `forward` function, during training we want to randomize the `mask` you just applied to the inputs. \n",
    "For that, please complete the `generate_random_mask` function that should return a random mask where True = masked-out and False = not masked.\n",
    "Each sample in the batch should randomly mask out between 1 and L tokens, where L is the sequence length. \n",
    "When L tokens are masked-out, it means there is no input and all tokens are predicted.\n",
    "When only 1 token is masked-out, it means that all but one token are given as input, and only one is predicted. \n",
    "You should be able to see why we have to have at least one token masked.\n",
    "\n",
    "The returned mask tensor should be of type `torch.BoolTensor`, moved to the same device (GPU) as `seq`, and be of shape (B, L).\n",
    "Note that both the number of mask tokens, as well as the placement of the masks should be sampled completely uniformly at random, for every sample in the batch individually.\n",
    "That means you should not apply and broadcast the same mask to the entire batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533221b-9754-4f89-b8d6-a8ddc105bce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1.4 MaskGIT schedule and generation function (20 points)\n",
    "\n",
    "Now let's implement the generation function. We do that in two steps:\n",
    "\n",
    "First, let's implement a so-called generation schedule in `get_maskgit_schedule`. \n",
    "Its task is to give us a list of number of tokens to unmask at every prediction step. \n",
    "It's quite common to make this a cosine schedule, i.e. where the tokens are unmasked slowly at the beginning, then in the middle many tokens are predicted at once, and finally in the end we ramp down again.\n",
    "Here we will implement a much simpler constant schedule, where the number of unmasked tokens per step is constant.\n",
    "For example, if total_tokens = 17 and num_steps = 8, then the schedule should be: [2, 2, 2, 2, 2, 2, 2, 3]. \n",
    "If the total number of tokens is not divisible by the number of steps, we simply add the remainder to the last step.\n",
    "The `schedule` should be a list of integers of length `num_steps`, where each integer represents the number of tokens to unmask at that step. \n",
    "The sum of the integers in `schedule` should equal `total_tokens`.\n",
    "\n",
    "With the simple schedule implemented, let's use it in the `generate` function. Generation is performed in a loop in the following steps:\n",
    "1. Given the sequence and mask so far, simply pass them through the network to get the logits.\n",
    "2. Then, select the subset of logits that we actually want to predict, i.e. the masked-out tokens.\n",
    "3. Over all these predicted tokens, we only want to keep the most \"confident\" predictions. We select for these by computing the maximum logit value for each token as a proxy. The higher the maximum logit is for a given token, the more \"confident\" it is in its prediction.\n",
    "4. Now, let's select the top-k tokens according to these confidence scores. You get the number of tokens `k` from the generation schedule.\n",
    "5. Sample the token indices from these `k` selected token logits. You should use the `sample_tokens` function from `utils/sampling.py`, and remember to pass the relevant sampling hyperparameters.\n",
    "6. Update the sequence and mask for the next round using the newly sampled tokens and their positions.\n",
    "7. Repeat until the end of the generation schedule, when the sequence is fully unmasked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd64834-6658-4cf1-a88d-aed19c61be1e",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoMaskGIT/mnist_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 1xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=1 run_training.py --config cfgs/nanoMaskGIT/mnist_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should be pretty fast and only take a few minutes. Because masked image models are harder to overfit, we increased the number of training steps five-fold, compared to nanoGPT. You should reach a final validation loss below 0.57, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoMaskGIT_mnist.png\" alt=\"nanoMaskGIT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a95694-bc7d-4ac5-964a-3e0a0e010d5d",
   "metadata": {},
   "source": [
    "### 2.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoMaskGIT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7567ebf-ca13-4ad5-a16b-8e2c1df0c935",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6e1bb-d198-4f3f-93b1-aed5cf5eb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './outputs/nanoMaskGIT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f899e5d6-dd87-4c07-8f40-351c49ff3524",
   "metadata": {},
   "source": [
    "Let's plot some class-conditional generations! We seed the generation by providing the first token, whose index is equal to the number we'd like to generate.\n",
    "For that token, we initialize the mask with `False`, i.e. indicating that that token is given as input, i.e. not masked. \n",
    "The rest of the tokens are masked, and it does not matter what value they have in the `seq` tensor, as they are overwritten by the learnable mask token in the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e298bb3-535f-4f70-90f4-0e8af217c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 5\n",
    "\n",
    "seq = torch.zeros(50, dtype=torch.long, device=device)\n",
    "mask = torch.ones(50, dtype=torch.bool, device=device)\n",
    "seq[0] = label\n",
    "mask[0] = False\n",
    "\n",
    "output = model.generate(seq, mask, num_steps=8, temp=1.0, top_p=0.9, return_history=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d1cde-f9ae-4bc8-b66f-cc6a89346721",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "plt.imshow(reconst[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da2fcd-f7cc-479a-8124-c7c52f0e0332",
   "metadata": {},
   "source": [
    "Let's now generate 10 random samples for all 10 classes. Most should look quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37a578-6b9f-4a62-bc36-bb9345c3b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, num_steps=8, temp=1.0, top_p=0.0, top_k=0.0, n_samples=10):\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(10, n_samples), axes_pad=0.1)\n",
    "    for label in range(10):\n",
    "        for sample_idx in range(n_samples):\n",
    "            grid_idx = label * n_samples + sample_idx\n",
    "            \n",
    "            seq = torch.zeros(50, dtype=torch.long, device=device)\n",
    "            mask = torch.ones(50, dtype=torch.bool, device=device)\n",
    "            seq[0] = label\n",
    "            mask[0] = False\n",
    "            output = model.generate(seq, mask, num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k, return_history=False)\n",
    "            \n",
    "            reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "            grid[grid_idx].imshow(reconst[0], cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "generate_samples(model, num_steps=8, temp=0.7, top_p=0.9, top_k=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c77608-5cb3-4f7e-9ffc-fd36aef83cdb",
   "metadata": {},
   "source": [
    "### 2.5 Open-ended questions (10 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ae3d6-ddaa-44d9-b5d8-9748a2ec3b5f",
   "metadata": {},
   "source": [
    "#### 2.5.1 Intermediate generation steps\n",
    "\n",
    "`model.generate` has an optional flag `return_history`. Show the intermediate generation steps. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5015471-fa5b-41bc-a190-ed5930e2d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb60ba-521e-43cd-9c01-c94507740fa4",
   "metadata": {},
   "source": [
    "#### 2.5.2 Number of inference steps\n",
    "\n",
    "With MaskGIT, we can freely choose the number of inference steps `k`. We default to `k=8`, but how does generation with `k` = 1, 4, 8, 16, 32, 49 perform? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08d6c1-4570-4d92-ade9-c59a4e8fc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6959e3e-6682-4b9e-afd4-d20362086f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3 Training nanoGPT on TinyStories\n",
    "\n",
    "Masked generation is quite common for image generation, but has seen a recent resurgence for language models too (e.g. see [LLaDA](https://ml-gsai.github.io/LLaDA-demo/)). \n",
    "Let's run a little experiment and train a masked model on TinyStories, just as we did with nanoGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f284f-4ffb-4611-98c1-302b09cd516a",
   "metadata": {},
   "source": [
    "### 3.1 Loading the tokenizer and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa2c0e-71af-4d51-9716-9ab4ca17f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "\n",
    "# Add padding, start-of-sequence, and end-of-sequence tokens\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '[SOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "})\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[('[EOS]', tokenizer.eos_token_id), ('[SOS]', tokenizer.bos_token_id)],\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08407185-c1f8-48fe-9bc8-5b0d35c33e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, text_tokenizer):\n",
    "    \"\"\" Helper function to turn token sequences back to well-formatted text. \"\"\"\n",
    "    decoded = text_tokenizer.decode(token_ids)\n",
    "    # Remove [SOS], [EOS], and [PAD] tokens along with surrounding horizontal whitespace only.\n",
    "    decoded = re.sub(r'[ \\t]*\\[(SOS|EOS|PAD)\\][ \\t]*', ' ', decoded)\n",
    "    # Collapse extra horizontal spaces in each line without touching newline characters.\n",
    "    decoded = '\\n'.join([re.sub(r'[ \\t]+', ' ', line).strip() for line in decoded.splitlines()])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e1ee7-7805-42ca-a5f7-7a9f45dadb9e",
   "metadata": {},
   "source": [
    "### 3.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoMaskGIT/tinystories_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 2xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 run_training.py --config cfgs/nanoMaskGIT/tinystories_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should take over one hour. You should reach a final validation loss around 2.05, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoMaskGIT_tinystories.png\" alt=\"nanoMaskGIT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e772-641d-401f-8759-d470913c07a8",
   "metadata": {},
   "source": [
    "### 3.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "[Note] Your screenshot must clearly show your Weights & Biases (W&B) account (username/entity), usually visible in the top-right corner of the page.\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoMaskGIT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae662ed8-4d89-419f-9419-72dfe1539a8b",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461ee865-a0a8-47d1-ac34-577a16c77fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './outputs/nanoMaskGIT/tinystories_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b76935-50bd-4467-b72d-30955eb516b7",
   "metadata": {},
   "source": [
    "Let's generate some random (unconditional) stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25761c-ae44-4e67-8c77-505d6be35a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    seq = torch.zeros(256, dtype=torch.long, device=device)\n",
    "    mask = torch.ones(256, dtype=torch.bool, device=device)\n",
    "    output = model.generate(seq, mask, num_steps=128, temp=1.0, top_k=100, return_history=False)\n",
    "    print(token_ids_to_text(output[0], text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb8ab8-0a6b-446c-af41-acacf2c584cd",
   "metadata": {},
   "source": [
    "### 3.5 Open-ended questions (10 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c1f71-4a23-4eb9-8f06-2854fc66b480",
   "metadata": {},
   "source": [
    "#### 3.5.1 Intermediate generation steps\n",
    "\n",
    "Similar as in 2.5.1, `model.generate` has an optional flag `return_history`. Show the intermediate generation steps. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a974f-773a-4851-a3d4-f3ded2a45022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85f348-ce6a-4cf7-8ba8-ae5ba0cacf6f",
   "metadata": {},
   "source": [
    "#### 3.5.2 Number of inference steps\n",
    "\n",
    "Similar as in 2.5.1, with MaskGIT, we can freely choose the number of inference steps `k`. We default to `k=8`, but how does generation with `k` = 1, 4, 8, 16, 32, 49 perform? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cad67-9e6e-47fd-8a8a-e43161548927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6b31c-2d8b-41f9-a124-70e0238daa06",
   "metadata": {},
   "source": [
    "#### 3.5.3 Comparison to autoregressive generation\n",
    "\n",
    "How would you compare these results to the ones you got from nanoGPT? What are some failure modes you observe? And what could be some benefits? Do you have any thoughs on how we can improve text generation with masked models, or should we just stick to autoregressive models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05978d-7fa1-4263-8179-ec11a46842c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf37ad-3f51-443d-879f-6632cea55f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 Further reading\n",
    "\n",
    "Here is some further reading material should you want to dive deeper on masked modeling.\n",
    "\n",
    "Masked image generation:\n",
    "- [MaskGIT: Masked Generative Image Transformer](https://arxiv.org/abs/2202.04200)\n",
    "- [Muse: Text-To-Image Generation via Masked Generative Transformers](https://arxiv.org/abs/2301.00704)\n",
    "- [MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis](https://arxiv.org/abs/2211.09117)\n",
    "- [Randomized Autoregressive Visual Generation](https://arxiv.org/abs/2411.00776)\n",
    "- [RandAR: Decoder-only Autoregressive Visual Generation in Random Orders](https://arxiv.org/abs/2412.01827)\n",
    "- [Autoregressive Image Generation without Vector Quantization](https://arxiv.org/abs/2406.11838)\n",
    "- [4M: Massively Multimodal Masked Modeling](https://arxiv.org/abs/2312.06647)\n",
    "\n",
    "Masked text generation:\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n",
    "- [Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006)\n",
    "- [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992)\n",
    "\n",
    "Masked pre-training:\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
    "- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n",
    "- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)\n",
    "- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)\n",
    "- [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)\n",
    "- [BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers](https://arxiv.org/abs/2208.06366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1dead-53d5-4b38-9ef3-52a7cbe47ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano4M kernel (nanofm)",
   "language": "python",
   "name": "nanofm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
